{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import PageElement\n",
    "from pydantic import BaseModel\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-20 18:35:02.726] [INFO] Logger setup complete\n"
     ]
    }
   ],
   "source": [
    "class CustomFormatter(logging.Formatter):\n",
    "    log_format = \"[%(asctime)s.%(msecs)03d] [%(levelname)s] %(message)s\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(self.log_format, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# remove any existing handlers to prevent double logging\n",
    "if logging.getLogger().hasHandlers():\n",
    "    logging.getLogger().handlers.clear()\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(CustomFormatter())\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "def log_func(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logger.info(f\"[{func.__name__}]\")\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "logger.info(\"Logger setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(BaseModel):\n",
    "    caption: Optional[str]  # Caption can be None or a string\n",
    "    comments: Optional[List[Optional[str]]]  # Comments can be None or a list of strings\n",
    "\n",
    "\n",
    "class DatasetModel(BaseModel):\n",
    "    data: Dict[str, Post]  # URL keys with Post values\n",
    "    author: Optional[str] = (\n",
    "        \"Putu Widyantara Artanta Wibawa\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data_dict=None):\n",
    "        \"\"\"Initialize with a dictionary and store it in self.data after validation.\"\"\"\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        self.data = DatasetModel(data=data_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_file):\n",
    "        \"\"\"Load JSON file, validate it, and return a new Dataset instance.\"\"\"\n",
    "        try:\n",
    "            with open(json_file, \"r\") as file:\n",
    "                json_data = json.load(file)\n",
    "                return cls(data_dict=json_data.get(\"data\", {}))\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: {json_file} not found.\")\n",
    "            return cls()\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"Error: Could not decode JSON from {json_file}.\")\n",
    "            return cls()\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Validation error: {e}\")\n",
    "            return cls()\n",
    "\n",
    "    def to_json(self, json_file):\n",
    "        \"\"\"Save self.data to a JSON file.\"\"\"\n",
    "        with open(json_file, \"w\") as file:\n",
    "            json.dump(self.data.model_dump(), file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_BASE_URL = \"https://x.com\"\n",
    "INSTAGRAM_BASE_URL = \"https://www.instagram.com\"\n",
    "FACEBOOK_BASE_URL = \"https://www.facebook.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-18 12:08:22.757] [WARNING] Error sending stats to Plausible: error sending request for url (https://plausible.io/api/event)\n"
     ]
    }
   ],
   "source": [
    "webdriver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver = Chrome()\n",
    "webdriver.get(\"https://www.instagram.com/\")\n",
    "\n",
    "# need login first, so wait for user to login\n",
    "# time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def show_first_post(url: str):\n",
    "    try:\n",
    "        webdriver.get(url)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\n",
    "            \"div\",\n",
    "            class_=\"x1lliihq x1n2onr6 xh8yej3 x4gyw5p xfllauq xo2y696 x11i5rnm x2pgyrj\",\n",
    "        )\n",
    "        list_urls = []\n",
    "\n",
    "        for div in divs:\n",
    "            a_tag = div.find(\"a\", recursive=False)\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                list_urls.append(a_tag[\"href\"])\n",
    "\n",
    "        element = webdriver.find_element(By.XPATH, f'//a[@href=\"{list_urls[0]}\"]')\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_caption() -> Optional[str]:\n",
    "    try:\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"_a9zs\")\n",
    "        for div in divs:\n",
    "            h1_tag = div.find(\n",
    "                \"h1\", class_=\"_ap3a _aaco _aacu _aacx _aad7 _aade\", recursive=False\n",
    "            )\n",
    "            for br in h1_tag.find_all(\"br\"):\n",
    "                br.replace_with(\"\\n\")\n",
    "            if h1_tag:\n",
    "                return h1_tag.text\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_element_xpath(element: WebElement) -> Optional[str]:\n",
    "    try:\n",
    "        full_xpath = webdriver.execute_script(\n",
    "            \"\"\"\n",
    "            function getElementXPath(element) {\n",
    "                if (element.id !== '') {\n",
    "                    return 'id(\"' + element.id + '\")';\n",
    "                }\n",
    "                if (element === document.body) {\n",
    "                    return element.tagName.toLowerCase();\n",
    "                }\n",
    "\n",
    "                let ix = 0;\n",
    "                const siblings = element.parentNode.childNodes;\n",
    "                let sameTagSiblings = 0;\n",
    "\n",
    "                for (let i = 0; i < siblings.length; i++) {\n",
    "                    if (siblings[i].nodeType === 1 && siblings[i].tagName === element.tagName) {\n",
    "                        sameTagSiblings++;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                for (let i = 0; i < siblings.length; i++) {\n",
    "                    const sibling = siblings[i];\n",
    "                    if (sibling === element) {\n",
    "                        let text = \"\";\n",
    "                        \n",
    "                        if (sameTagSiblings > 1) {\n",
    "                            text = '[' + (ix + 1) + ']';\n",
    "                        }\n",
    "                        \n",
    "                        return getElementXPath(element.parentNode) + '/' + element.tagName.toLowerCase() + text;\n",
    "                    }\n",
    "\n",
    "                    if (sibling.nodeType === 1 && sibling.tagName === element.tagName) {\n",
    "                        ix++;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return getElementXPath(arguments[0]);\n",
    "\n",
    "        \"\"\",\n",
    "            element,\n",
    "        )\n",
    "        result = f\"/html/{full_xpath}\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def load_more_comments():\n",
    "    try:\n",
    "        title = webdriver.find_element(\n",
    "            By.XPATH, \"//*[contains(text(), 'Load more comments')]\"\n",
    "        )\n",
    "        if title:\n",
    "            is_found = True\n",
    "            while is_found:\n",
    "                try:\n",
    "                    title = webdriver.find_element(\n",
    "                        By.XPATH, \"//*[contains(text(), 'Load more comments')]\"\n",
    "                    )\n",
    "                    title_xpath = _get_element_xpath(title)\n",
    "                    button_xpath = title_xpath[\n",
    "                        : title_xpath.rfind(\"button\") + len(\"button\")\n",
    "                    ]\n",
    "                    try:\n",
    "                        button_element = webdriver.find_element(By.XPATH, button_xpath)\n",
    "                        button_element.click()\n",
    "                    except Exception as e:\n",
    "                        logger.error(str(e).split(\"\\n\")[0])\n",
    "                        is_found = False\n",
    "                except Exception as e:\n",
    "                    logger.error(str(e).split(\"\\n\")[0])\n",
    "                    is_found = False\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def show_replies():\n",
    "    try:\n",
    "\n",
    "        button = webdriver.find_elements(\n",
    "            \"xpath\", \"//button[contains(@class, '_acan _acao _acas _aj1- _ap30')]\"\n",
    "        )\n",
    "        result_button = [\n",
    "            b\n",
    "            for b in button\n",
    "            if (b.text.startswith(\"View replies\") or b.text.startswith(\"View all\"))\n",
    "        ]\n",
    "        total_button = len(result_button)\n",
    "        if total_button > 0:\n",
    "            for b in result_button:\n",
    "                b.click()\n",
    "            logger.info(f\"Total button clicked: {total_button}\")\n",
    "        else:\n",
    "            logger.warning(\"No replies found\")\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_comments() -> list[str]:\n",
    "    try:\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        comments = soup.find_all(\"div\", class_=\"_a9zs\")\n",
    "        result = []\n",
    "        for div in comments:\n",
    "            span_tag = div.find(\n",
    "                \"span\", class_=\"_ap3a _aaco _aacu _aacx _aad7 _aade\", recursive=False\n",
    "            )\n",
    "            if span_tag:\n",
    "                result.append(span_tag.text)\n",
    "        logger.info(f\"Total comments found: {len(result)}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def next_post():\n",
    "    try:\n",
    "        button = webdriver.find_element(\n",
    "            By.XPATH,\n",
    "            f'//span[@style=\"display: inline-block; transform: rotate(90deg);\"]',\n",
    "        )\n",
    "        button.click()\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "\n",
    "@log_func\n",
    "def has_next_post() -> bool:\n",
    "    try:\n",
    "        webdriver.find_element(\n",
    "            By.XPATH,\n",
    "            f'//span[@style=\"display: inline-block; transform: rotate(90deg);\"]',\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _get_single_post_data() -> Post:\n",
    "    load_more_comments()\n",
    "    show_replies()\n",
    "    caption = get_caption()\n",
    "    comments = get_comments()\n",
    "    return Post(caption=caption, comments=comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_instagram(username: str, post: Optional[int] = -1) -> Dataset:\n",
    "    try:\n",
    "        if post == 0:\n",
    "            return\n",
    "\n",
    "        result = Dataset()\n",
    "        url = f\"https://www.instagram.com/{username}/\"\n",
    "        show_first_post(url)\n",
    "\n",
    "        # get data\n",
    "        post_data = _get_single_post_data()\n",
    "        # 'https://www.instagram.com/p/:POST_ID/?img_index=1'\n",
    "        post_id = webdriver.current_url\n",
    "        result.data.data.update({post_id: post_data})\n",
    "\n",
    "        if post == -1:\n",
    "            while has_next_post():\n",
    "                next_post()\n",
    "                time.sleep(2)\n",
    "                post_data = _get_single_post_data()\n",
    "                post_id = webdriver.current_url\n",
    "                result.data.data.update({post_id: post_data})\n",
    "        else:\n",
    "            while post and has_next_post():\n",
    "                next_post()\n",
    "                post -= 1\n",
    "                time.sleep(2)\n",
    "                post_data = _get_single_post_data()\n",
    "                post_id = webdriver.current_url\n",
    "                result.data.data.update({post_id: post_data})\n",
    "            if post:\n",
    "                logger.warning(\"Total post less than expected\")\n",
    "\n",
    "        # stats\n",
    "        total_post = len(result.data.data)\n",
    "        total_comments = sum(\n",
    "            len(post.comments) for post in result.data.data.values()\n",
    "        )\n",
    "        logger.info(f\"Total post scraped: {total_post}\")\n",
    "        logger.info(f\"Total comments scraped: {total_comments}\")\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return Dataset(data={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-17 22:06:13.642] [INFO] [show_first_post]\n",
      "[2024-10-17 22:06:17.310] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:17.312] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:17.339] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:17.340] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:17.384] [WARNING] No replies found\n",
      "[2024-10-17 22:06:17.385] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:17.900] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:18.430] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:18.439] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:18.453] [INFO] [next_post]\n",
      "[2024-10-17 22:06:20.585] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:20.586] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:20.604] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:20.605] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:20.704] [INFO] Total button clicked: 1\n",
      "[2024-10-17 22:06:20.704] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:21.105] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:21.550] [INFO] Total comments found: 4\n",
      "[2024-10-17 22:06:21.562] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:21.594] [INFO] [next_post]\n",
      "[2024-10-17 22:06:23.757] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:23.758] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:23.781] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:23.783] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:23.845] [WARNING] No replies found\n",
      "[2024-10-17 22:06:23.846] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:24.299] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:24.592] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:24.599] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:24.606] [INFO] [next_post]\n",
      "[2024-10-17 22:06:26.706] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:26.707] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:26.728] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:26.729] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:26.775] [WARNING] No replies found\n",
      "[2024-10-17 22:06:26.776] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:27.188] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:27.612] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:27.622] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:27.633] [INFO] [next_post]\n",
      "[2024-10-17 22:06:29.734] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:29.734] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:29.753] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:29.754] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:29.826] [INFO] Total button clicked: 1\n",
      "[2024-10-17 22:06:29.827] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:30.286] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:30.666] [INFO] Total comments found: 3\n",
      "[2024-10-17 22:06:30.672] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:30.681] [INFO] [next_post]\n",
      "[2024-10-17 22:06:32.775] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:32.777] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:32.980] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:32.981] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:33.128] [WARNING] No replies found\n",
      "[2024-10-17 22:06:33.129] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:33.815] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:34.490] [INFO] Total comments found: 24\n",
      "[2024-10-17 22:06:34.500] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:34.513] [INFO] [next_post]\n",
      "[2024-10-17 22:06:36.721] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:36.722] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:36.739] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:36.739] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:36.780] [WARNING] No replies found\n",
      "[2024-10-17 22:06:36.781] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:37.463] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:37.928] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:37.937] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:37.948] [INFO] [next_post]\n",
      "[2024-10-17 22:06:40.106] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:40.106] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:40.125] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:40.126] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:40.138] [WARNING] No replies found\n",
      "[2024-10-17 22:06:40.139] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:40.740] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:41.328] [INFO] Total comments found: 4\n",
      "[2024-10-17 22:06:41.339] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:41.352] [INFO] [next_post]\n",
      "[2024-10-17 22:06:43.500] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:43.500] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:43.523] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:43.524] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:43.534] [WARNING] No replies found\n",
      "[2024-10-17 22:06:43.535] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:44.081] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:44.603] [INFO] Total comments found: 1\n",
      "[2024-10-17 22:06:44.647] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:44.663] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//span[@style=\"display: inline-block; transform: rotate(90deg);\"]\"}\n",
      "[2024-10-17 22:06:44.664] [INFO] Total post scraped: 9\n",
      "[2024-10-17 22:06:44.664] [INFO] Total comments scraped: 36\n"
     ]
    }
   ],
   "source": [
    "instagram_dataset = scraping_instagram(\"putu_waw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://www.instagram.com/p/C3W-SslrQpp/': {'caption': 'Halo seluruh mahasiswa Indonesia. Saya siap mengikuti Magang dan Studi Independen Bersertifikat Angkatan 6!',\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/Cv4iiXPLDl6/': {'caption': 'Halo! Saya Putu Widyantara Artanta Wibawa dari Universitas Udayana siap mengikuti National Onboarding MSIB Angkatan 5!\\n\\n#BerprosesLebihBaik #KampusMerdeka #MSIB5 #MagangMerdeka #MagangBersertifikat #BukanMagangdanStudiBiasa #MSIB5',\n",
       "   'comments': ['Mangaaat', 'ðŸ”¥', 'Great My sonðŸ˜', 'Semangat frennðŸ”¥']},\n",
       "  'https://www.instagram.com/p/CZqhWPWlNYN/': {'caption': '[SAYA SIAP MENGIKUTI MAHASISYA UPANAYANA XIX]\\n\\nOm Swastyastu ðŸ™\\n\"Om Ano Bhadrah Kratavo Yantu Visvatah\" - (Yajur Veda XXV. 14)\\n(Semoga pikiran yang baik datang dari segala penjuru)\\n\\nMahasisya Upanayana merupakan upacara penyucian diri dengan tujuan memohon doa restu secara niskala tatkala seorang mahasiswa akan menuntut ilmu dan berguru di Universitas Udayana.\\n\\nSaya Putu Widyantara Artanta Wibawa, Siap mengikuti Mahasisya Upanayana XIX tahun 2022. \\n\\n\"Tad viddhi praÅ†ipÄtena\\nParipraÅ›neÅ†a sÄ“vayÄ\\nUpadekÅŸyanti te jÃ±Änam\\nJÃ±Äninas tattva darÅ›inah\"\\n(Bhagavadgita IV. 34)\\n\\n(Kejarlah kebijakan itu dengan kerendahan hati, dengan bertanya-tanya dan dengan pengabdian. Orang bijaksana yang melihat kebenaran itu akan memberi petunjuk padamu tentang pengetahuan itu).\\n\\nðŸ™ðŸ» Om Santih, Santih, Santih Om ðŸ™ðŸ»\\nSatyam Eva Jayate!',\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/CStykH6lYRU/': {'caption': \"[I'M READY FOR STUDENT DAY FMIPA 2021]\\n\\nHalo, sobat MIPA!ðŸ‘‹\\nPerkenalkan saya Putu Widyantara Artanta Wibawa, mahasiswa Program Studi Informatika siap mengikuti Student Day FMIPA 2021. Sampai jumpa di Hari Puncak Student Day FMIPA 2021 pada tanggal 20 Agustus 2021.\\n\\nSaya MIPA, Saya Bangga!\\n\\n@bemfmipaunud @fmipaunud\\n#StudentDayFMIPAUnud2021\\n#MIPAJaya\\n#ProudToBeSilver\",\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/CSgDdyaLOuL/': {'caption': '[PKKMB FMIPA]\\n\\nSaya Putu Widyantara Artanta Wibawa, dari Fakultas Matematika dan Ilmu Pengetahuan Alam, Prodi Informatika.\\n\\nSaya siap mengikuti PKKMB FMIPA pada tanggal 18-19 Agustus 2021.\\n\\n@fmipaunud\\n#fmipaUnud\\n#FakultasMIPAUniversitasUdayana\\n#PKKMBFMIPA2021\\n#PKKMBUNUD2021\\n#PKKMB2021',\n",
       "   'comments': ['Kepak sayap FMIPAðŸ”¥',\n",
       "    'Jurusan apa kau? Semangat yooo',\n",
       "    '@prabhageg ngambil jurusan ilmu komputer. Siapp, thanks prabhaðŸ™Œ']},\n",
       "  'https://www.instagram.com/p/CSbwvz_lyUK/': {'caption': '[Saya Ksatria Muda Udayana Siap Mengikuti Student Day 2021]\\n\\nJalan-jalan ke Kota Tua\\nDi Kota Tua ada Car-Free Day\\nMohon izin kakak-kakak semua\\nSaya siap ikuti Student Day\\n\\nSaya Putu Widyantara Artanta Wibawa siap mengikuti Student Day 2021 Universitas Udayana pada tanggal 12-13 Agustus 2021.\\n\\n#StudentDay2021Unud\\n#ParwataArundaya',\n",
       "   'comments': ['Semangat putuðŸ”¥ðŸ”¥',\n",
       "    'mangadd kak putuwwðŸ”¥',\n",
       "    'Semangat cokkkðŸ”¥',\n",
       "    'keren tuuðŸ”¥',\n",
       "    'mangatt putu!ðŸ”¥ðŸ™Œ',\n",
       "    'Semangatt putuuðŸ”¥ðŸ”¥',\n",
       "    'mangat cak caknyaðŸ”¥ðŸ”¥',\n",
       "    'Semangat weeeðŸ™ŒðŸ”¥',\n",
       "    'Mangats waww',\n",
       "    'Mangattt wawðŸ”¥ðŸ”¥',\n",
       "    'Inget kiri-kanan TuuðŸ™ðŸ¼ðŸ¤ðŸ»',\n",
       "    'Ow semangat ðŸ”¥',\n",
       "    'Semangat bang ðŸ”¥',\n",
       "    'semangaatt putuuðŸ™ŒðŸ”¥',\n",
       "    'MANGAT WAWðŸ”¥',\n",
       "    'Mangattt putuuu!ðŸ™Œ',\n",
       "    'Semangat putuu ðŸ”¥',\n",
       "    'MANGATTT PUTU WAWWWWðŸ”¥ðŸ”¥',\n",
       "    'semangat tuu',\n",
       "    'Mangaaattsss',\n",
       "    'Semangat tuðŸ”¥',\n",
       "    'semangattt broo ðŸ¤©',\n",
       "    'ðŸ”¥ðŸ”¥ðŸ”¥',\n",
       "    'semangatt waw!']},\n",
       "  'https://www.instagram.com/p/B4Uyx6FlxyM/': {'caption': 'TPF20192410_007_Putu Widyantara Artanta Wibawa_Sore-Sore Ring Pelabuhan Buleleng_Objek Wisata di Bali\\n\\nSore-sore ring pelabuhan Buleleng\\nPemandangan yukti lintang asri\\nOmbak ane menepi pesisi\\nRikala Sang Surya jagi pineleb.\\nKutipan lagu karya Gede Darma ini menjadi saksi keindahan Eks Pelabuhan Buleleng yang pernah menjadi dermaga terbesar di Pulau Bali sebelum pusat pemerintah Provinsi Bali dipindah ke Bali Selatan.\\n\\nDi kawasan Eks Pelabuhan Buleleng dibangun monumen Yudha Mandala untuk mengenang perjuangan rakyat Bali disaat berjuang melawan  penjajahan Belanda. \\nKayu-kayu bekas yang berumur tua di dermaga ini dirubah menjadi restoran terapung yang memiliki desain yang unik dengan panorama pantai yang berpadu dengan deburan ombak serta semilir angin yang berhembus lembut menerpa tubuh.\\n\\nInilah Eks Pelabuhan Buleleng yang sekarang, dengan sajian keindahannya yang begitu memesona dan sarat akan nilai sejarah. [Keterangan Teknis]\\nDevice : OPPO F7\\nFocal Length : 3.62 mm\\nAperture : f/1.8\\nISO : 40\\nFlash : No flash\\nWhite Balance : Auto\\nExposure Time : 1/374 s\\n[Tags]\\n@odowkun_ @pringga_arts @rosyanaputra_ @hima_tp_undiksha #tpfotografi2019 #himatpundiksha',\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/BRhg04YjO0w/': {'caption': 'Landscape Photography by : @putu_waw. This photo took on Sunday, 12 March 2017 at 06.20 (GMT + 8).',\n",
       "   'comments': ['So nicee',\n",
       "    '#travel',\n",
       "    'Pretty cool!',\n",
       "    'GREAT TRAVEL...Like it a lot ðŸ¤—.']},\n",
       "  'https://www.instagram.com/p/BPwIVQzBJ6t/': {'caption': 'Landscape Photography by : @putu_waw. This photo took on Wednesday, 14 December 2016 at 05 : 53 (GMT + 8).',\n",
       "   'comments': ['ðŸ‘Œ']}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa'}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagram_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dataset = Dataset.from_json(\"aneh.json\")\n",
    "# current_dataset.data.data.update(instagram_dataset.data.data)\n",
    "# current_dataset.to_json(\"aneh.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver = Chrome()\n",
    "webdriver.get(\"https://x.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _scraping_profile_tweet(dataset: Dataset, post: Optional[int] = -1):\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "\n",
    "        url_list = []\n",
    "        div_elements = soup.find_all(\"div\", class_=\"css-175oi2r r-18u37iz r-1q142lx\")\n",
    "        for div_element in div_elements:\n",
    "            a_tag = div_element.find(\"a\", recursive=False)\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                url_list.append(a_tag[\"href\"])\n",
    "\n",
    "        min_idx = min(len(divs), len(url_list))\n",
    "        for idx in range(min_idx):\n",
    "            url = f\"{TWITTER_BASE_URL}{url_list[idx]}\"\n",
    "            dataset.data.data.update(\n",
    "                {url: Post(caption=divs[idx].text, comments=[])}\n",
    "            )\n",
    "\n",
    "        length_data = len(dataset.data.data)\n",
    "        logger.info(f\"Total tweets scraped: {length_data}\")\n",
    "        if post != -1 and length_data >= post:\n",
    "            break\n",
    "        history.append(length_data)\n",
    "\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new tweets found\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _scraping_tweet_comment():\n",
    "    result = list()\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "\n",
    "        for div in divs:\n",
    "            if div.text not in result:\n",
    "                result.append(div.text)\n",
    "\n",
    "        logger.info(f\"Total comment tweets scraped: {len((result))}\")\n",
    "        history.append(len(result))\n",
    "\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new comment found\")\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_twitter(username: str, post: Optional[int] = -1) -> Dataset:\n",
    "    dataset = Dataset()\n",
    "    webdriver.get(f\"https://x.com/{username}\")\n",
    "    _scraping_profile_tweet(dataset, post)\n",
    "\n",
    "    for url in dataset.data.data.keys():\n",
    "        webdriver.get(f\"{url}\")\n",
    "        logger.info(f\"Scraping comments for tweet: {url}\")\n",
    "        time.sleep(4)\n",
    "\n",
    "        comments = _scraping_tweet_comment()\n",
    "\n",
    "        current_caption = dataset.data.data.get(url).caption\n",
    "        comments.remove(current_caption)\n",
    "\n",
    "        logger.info(f\"Final comments scraped: {len(comments)}\")\n",
    "        dataset.data.data.get(url).comments = comments\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-17 22:19:59.310] [INFO] [_scraping_profile_tweet]\n",
      "[2024-10-17 22:19:59.887] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:00.298] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:00.677] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:01.057] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:01.579] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:02.445] [INFO] Total tweets scraped: 3\n",
      "[2024-10-17 22:20:02.957] [INFO] Total tweets scraped: 3\n",
      "[2024-10-17 22:20:03.372] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:03.753] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:04.146] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:04.577] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:05.024] [INFO] Total tweets scraped: 5\n",
      "[2024-10-17 22:20:05.493] [INFO] Total tweets scraped: 6\n",
      "[2024-10-17 22:20:05.899] [INFO] Total tweets scraped: 6\n",
      "[2024-10-17 22:20:06.389] [INFO] Total tweets scraped: 7\n",
      "[2024-10-17 22:20:06.869] [INFO] Total tweets scraped: 7\n",
      "[2024-10-17 22:20:07.337] [INFO] Total tweets scraped: 8\n",
      "[2024-10-17 22:20:07.732] [INFO] Total tweets scraped: 8\n",
      "[2024-10-17 22:20:08.141] [INFO] Total tweets scraped: 9\n",
      "[2024-10-17 22:20:08.593] [INFO] Total tweets scraped: 11\n",
      "[2024-10-17 22:20:09.081] [INFO] Total tweets scraped: 12\n",
      "[2024-10-17 22:20:09.518] [INFO] Total tweets scraped: 12\n",
      "[2024-10-17 22:20:10.005] [INFO] Total tweets scraped: 13\n",
      "[2024-10-17 22:20:10.529] [INFO] Total tweets scraped: 13\n",
      "[2024-10-17 22:20:10.944] [INFO] Total tweets scraped: 14\n",
      "[2024-10-17 22:20:11.432] [INFO] Total tweets scraped: 15\n",
      "[2024-10-17 22:20:11.919] [INFO] Total tweets scraped: 15\n",
      "[2024-10-17 22:20:12.377] [INFO] Total tweets scraped: 16\n",
      "[2024-10-17 22:20:12.821] [INFO] Total tweets scraped: 16\n",
      "[2024-10-17 22:20:13.279] [INFO] Total tweets scraped: 17\n",
      "[2024-10-17 22:20:13.711] [INFO] Total tweets scraped: 17\n",
      "[2024-10-17 22:20:14.222] [INFO] Total tweets scraped: 18\n",
      "[2024-10-17 22:20:14.630] [INFO] Total tweets scraped: 18\n",
      "[2024-10-17 22:20:15.057] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:15.452] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:15.850] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:16.249] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:16.648] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:17.124] [INFO] Total tweets scraped: 21\n",
      "[2024-10-17 22:20:17.559] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:18.038] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:18.497] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:18.981] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:19.402] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:19.787] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:20.179] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:20.578] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:20.957] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:21.350] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:21.351] [INFO] No new tweets found\n",
      "[2024-10-17 22:20:21.656] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1747615071537361226\n",
      "[2024-10-17 22:20:25.658] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:26.017] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:26.364] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:26.717] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:27.077] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:27.510] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:27.859] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:28.212] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:28.567] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:28.922] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:29.275] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:29.633] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:29.634] [INFO] No new comment found\n",
      "[2024-10-17 22:20:29.634] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:20:29.795] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1747614627301867992\n",
      "[2024-10-17 22:20:33.797] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:34.156] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:34.507] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:34.860] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:35.209] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:35.565] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:35.921] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:36.265] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:36.631] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:36.982] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:37.334] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:37.686] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:37.687] [INFO] No new comment found\n",
      "[2024-10-17 22:20:37.687] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:20:37.833] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1747613877423206496\n",
      "[2024-10-17 22:20:41.835] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:42.199] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:42.554] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:42.907] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:43.275] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:43.711] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:44.059] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:44.436] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:44.811] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.161] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.509] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.866] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.867] [INFO] No new comment found\n",
      "[2024-10-17 22:20:45.867] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:20:46.010] [INFO] Scraping comments for tweet: https://x.com/streamlit/status/1690045031640375296\n",
      "[2024-10-17 22:20:50.012] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:50.390] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:50.757] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:51.110] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:51.471] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:51.827] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:52.190] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:52.555] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:52.915] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:53.270] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:53.716] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:54.070] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:54.071] [INFO] No new comment found\n",
      "[2024-10-17 22:20:54.072] [INFO] Final comments scraped: 2\n",
      "[2024-10-17 22:20:54.216] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1253887166217728006\n",
      "[2024-10-17 22:20:58.218] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:58.582] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:58.933] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:59.289] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:59.637] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:59.987] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:00.347] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:00.696] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:01.040] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:01.384] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:01.729] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:02.074] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:02.074] [INFO] No new comment found\n",
      "[2024-10-17 22:21:02.075] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:21:02.230] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1248838039314427905\n",
      "[2024-10-17 22:21:06.232] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:21:06.597] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:06.948] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:07.304] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:07.667] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:08.024] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:08.395] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:08.743] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:09.091] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:09.526] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:09.873] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:10.221] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:10.221] [INFO] No new comment found\n",
      "[2024-10-17 22:21:10.222] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:21:10.363] [INFO] Scraping comments for tweet: https://x.com/jokowi/status/1242396278106902529\n",
      "[2024-10-17 22:21:14.365] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:21:14.758] [INFO] Total comment tweets scraped: 10\n",
      "[2024-10-17 22:21:15.159] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:15.550] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:15.935] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:16.327] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:17.084] [INFO] Total comment tweets scraped: 20\n",
      "[2024-10-17 22:21:17.649] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:18.089] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:18.534] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:18.978] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:19.409] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:20.081] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:20.536] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:20.969] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:21.393] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:22.005] [INFO] Total comment tweets scraped: 41\n",
      "[2024-10-17 22:21:22.464] [INFO] Total comment tweets scraped: 41\n",
      "[2024-10-17 22:21:23.368] [INFO] Total comment tweets scraped: 45\n",
      "[2024-10-17 22:21:24.061] [INFO] Total comment tweets scraped: 46\n",
      "[2024-10-17 22:21:24.534] [INFO] Total comment tweets scraped: 46\n",
      "[2024-10-17 22:21:25.124] [INFO] Total comment tweets scraped: 48\n",
      "[2024-10-17 22:21:25.687] [INFO] Total comment tweets scraped: 48\n",
      "[2024-10-17 22:21:26.145] [INFO] Total comment tweets scraped: 48\n",
      "[2024-10-17 22:21:26.573] [INFO] Total comment tweets scraped: 49\n",
      "[2024-10-17 22:21:27.024] [INFO] Total comment tweets scraped: 50\n",
      "[2024-10-17 22:21:27.420] [INFO] Total comment tweets scraped: 50\n",
      "[2024-10-17 22:21:27.828] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:28.202] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:28.575] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:29.075] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:29.450] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:30.354] [INFO] Total comment tweets scraped: 60\n",
      "[2024-10-17 22:21:30.929] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:31.449] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:31.972] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:32.387] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:33.200] [INFO] Total comment tweets scraped: 67\n",
      "[2024-10-17 22:21:33.946] [INFO] Total comment tweets scraped: 70\n",
      "[2024-10-17 22:21:34.429] [INFO] Total comment tweets scraped: 70\n",
      "[2024-10-17 22:21:34.890] [INFO] Total comment tweets scraped: 70\n",
      "[2024-10-17 22:21:35.907] [INFO] Total comment tweets scraped: 80\n",
      "[2024-10-17 22:21:36.532] [INFO] Total comment tweets scraped: 80\n",
      "[2024-10-17 22:21:37.123] [INFO] Total comment tweets scraped: 80\n",
      "[2024-10-17 22:21:37.963] [INFO] Total comment tweets scraped: 85\n",
      "[2024-10-17 22:21:38.938] [INFO] Total comment tweets scraped: 90\n",
      "[2024-10-17 22:21:39.411] [INFO] Total comment tweets scraped: 90\n",
      "[2024-10-17 22:21:39.993] [INFO] Total comment tweets scraped: 90\n",
      "[2024-10-17 22:21:40.993] [INFO] Total comment tweets scraped: 97\n",
      "[2024-10-17 22:21:41.791] [INFO] Total comment tweets scraped: 97\n",
      "[2024-10-17 22:21:42.531] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:43.175] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:43.664] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:44.159] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:44.690] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:45.956] [INFO] Total comment tweets scraped: 110\n",
      "[2024-10-17 22:21:46.581] [INFO] Total comment tweets scraped: 110\n",
      "[2024-10-17 22:21:47.248] [INFO] Total comment tweets scraped: 110\n",
      "[2024-10-17 22:21:48.224] [INFO] Total comment tweets scraped: 118\n",
      "[2024-10-17 22:21:48.798] [INFO] Total comment tweets scraped: 119\n",
      "[2024-10-17 22:21:49.281] [INFO] Total comment tweets scraped: 119\n",
      "[2024-10-17 22:21:49.857] [INFO] Total comment tweets scraped: 119\n",
      "[2024-10-17 22:21:50.949] [INFO] Total comment tweets scraped: 126\n",
      "[2024-10-17 22:21:51.614] [INFO] Total comment tweets scraped: 128\n",
      "[2024-10-17 22:21:52.346] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:52.863] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:53.461] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:53.941] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:54.369] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:55.206] [INFO] Total comment tweets scraped: 138\n",
      "[2024-10-17 22:21:55.871] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:56.423] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:56.992] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:57.429] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:57.874] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:58.620] [INFO] Total comment tweets scraped: 148\n",
      "[2024-10-17 22:21:59.393] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:21:59.943] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:00.402] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:00.839] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:01.260] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:02.434] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:02.897] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:03.518] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:03.929] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:04.807] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:05.384] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:05.880] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:06.513] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:07.289] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:07.748] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:08.207] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:08.632] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:09.063] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:09.575] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:09.978] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:10.400] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:10.806] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:11.205] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:11.207] [INFO] No new comment found\n",
      "[2024-10-17 22:22:11.207] [INFO] Final comments scraped: 176\n",
      "[2024-10-17 22:22:11.527] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1220496942519554048\n",
      "[2024-10-17 22:22:15.529] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:15.888] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:16.234] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:16.594] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:16.962] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:17.313] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:17.659] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:18.103] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:18.449] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:18.796] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:19.153] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:19.503] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:19.504] [INFO] No new comment found\n",
      "[2024-10-17 22:22:19.504] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:19.647] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1216726296220225536\n",
      "[2024-10-17 22:22:23.648] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:23.996] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:24.354] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:24.705] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:25.052] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:25.402] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:25.747] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:26.099] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:26.450] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:26.805] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:27.149] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:27.496] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:27.497] [INFO] No new comment found\n",
      "[2024-10-17 22:22:27.497] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:27.649] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1212040819391123457\n",
      "[2024-10-17 22:22:31.651] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:32.001] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:32.352] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:32.698] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:33.046] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:33.392] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:33.736] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:34.175] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:34.523] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:34.871] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:35.229] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:35.583] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:35.583] [INFO] No new comment found\n",
      "[2024-10-17 22:22:35.584] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:35.730] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1208591386821320704\n",
      "[2024-10-17 22:22:39.732] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:40.093] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:40.447] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:40.791] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:41.142] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:41.493] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:41.838] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:42.186] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:42.541] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:42.889] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:43.238] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:43.584] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:43.585] [INFO] No new comment found\n",
      "[2024-10-17 22:22:43.586] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:43.724] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1205885544732868608\n",
      "[2024-10-17 22:22:47.726] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:48.089] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:48.460] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:48.808] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:49.168] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:49.516] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:49.980] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:50.340] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:50.707] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.055] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.407] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.753] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.753] [INFO] No new comment found\n",
      "[2024-10-17 22:22:51.754] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:51.900] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1159307146783154176\n",
      "[2024-10-17 22:22:55.902] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:56.263] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:56.621] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:56.966] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:57.324] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:57.670] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:58.017] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:58.367] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:58.713] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.061] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.409] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.754] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.754] [INFO] No new comment found\n",
      "[2024-10-17 22:22:59.755] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:59.897] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1151243345110294528\n",
      "[2024-10-17 22:23:03.899] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:04.255] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:04.606] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:04.949] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:05.302] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:05.733] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:06.081] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:06.427] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:06.774] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.129] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.477] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.822] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.823] [INFO] No new comment found\n",
      "[2024-10-17 22:23:07.823] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:07.965] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1143386205066629121\n",
      "[2024-10-17 22:23:11.966] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:12.328] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:12.691] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:13.052] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:13.399] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:13.757] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:14.109] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:14.459] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:14.817] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.173] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.522] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.875] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.876] [INFO] No new comment found\n",
      "[2024-10-17 22:23:15.876] [INFO] Final comments scraped: 1\n",
      "[2024-10-17 22:23:16.013] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1140272844858777602\n",
      "[2024-10-17 22:23:20.015] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:20.368] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:20.727] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:21.159] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:21.518] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:21.871] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:22.214] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:22.609] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:22.962] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.310] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.652] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.997] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.997] [INFO] No new comment found\n",
      "[2024-10-17 22:23:23.998] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:24.137] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1137016268580503553\n",
      "[2024-10-17 22:23:28.138] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:28.496] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:28.846] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:29.196] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:29.548] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:29.896] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:30.248] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:30.606] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:30.951] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:31.299] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:31.655] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:32.012] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:32.013] [INFO] No new comment found\n",
      "[2024-10-17 22:23:32.013] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:32.155] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1133278655046492161\n",
      "[2024-10-17 22:23:36.157] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:36.518] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:36.864] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:37.299] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:37.648] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:37.993] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:38.338] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:38.691] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:39.040] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:39.388] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:39.737] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:40.086] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:40.087] [INFO] No new comment found\n",
      "[2024-10-17 22:23:40.088] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:40.234] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1118875128782180353\n",
      "[2024-10-17 22:23:44.236] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:44.589] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:44.939] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:45.287] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:45.630] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:45.977] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:46.324] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:46.667] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:47.015] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:47.367] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:47.718] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:48.067] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:48.068] [INFO] No new comment found\n",
      "[2024-10-17 22:23:48.068] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:48.215] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1111973639123959808\n",
      "[2024-10-17 22:23:52.216] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:52.574] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:52.937] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:53.375] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:53.725] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:54.082] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:54.436] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:54.779] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:55.141] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:55.502] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:55.861] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:56.210] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:56.211] [INFO] No new comment found\n",
      "[2024-10-17 22:23:56.212] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:56.355] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1056487916334067713\n",
      "[2024-10-17 22:24:00.356] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:24:00.722] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:01.091] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:01.433] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:01.781] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:02.135] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:02.482] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:02.826] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:03.172] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:03.514] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:03.862] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:04.206] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:04.207] [INFO] No new comment found\n",
      "[2024-10-17 22:24:04.208] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:24:04.353] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1041716765611286528\n",
      "[2024-10-17 22:24:08.355] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:24:08.717] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:09.160] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:09.504] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:09.851] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:10.197] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:10.544] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:10.896] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:11.288] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:11.640] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:11.990] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:12.332] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:12.333] [INFO] No new comment found\n",
      "[2024-10-17 22:24:12.333] [INFO] Final comments scraped: 0\n"
     ]
    }
   ],
   "source": [
    "twitter_dataset = scraping_twitter(\"putu_waw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://x.com/putu_waw/status/1747615071537361226': {'caption': \"The waiting is over! I'm very happy because I have already get the final transcript for Bangkit 2023. I hope I can become one of the Bangkit distinct graduation \\n#lifeatbangkit\",\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1747614627301867992': {'caption': 'Finally, I completed 2 optional courses given by Banfkit about TensorFlow Advance Technique and NLP. Next is completing the Dicoding course hehe\\n\\n#lifeatbangkit',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1747613877423206496': {'caption': \"Hi everyone, I'm very happy to share with you that finally I completed all of the course at Bangkit. Letsgoo\\n#lifeatbangkit\",\n",
       "   'comments': []},\n",
       "  'https://x.com/streamlit/status/1690045031640375296': {'caption': \" Putu Widyantara Artanta Wibawa \\n\\n@putu_waw's @CockroachDB Connection! The demo app shows how to build the connection and query the database.\\n\\n Connection: https://buff.ly/4412DWs\\n App: https://buff.ly/4499NYt \\n\\n6/7\",\n",
       "   'comments': [\" Pedro Toledo \\n\\n@pedrotol_'s @trychroma Connection! The demo gives examples on how to use Chroma on two of the three available deployment modes, and instructions on how to test the third.\\n\\n Connection: https://buff.ly/444bypO\\n App: https://buff.ly/3YJp69z \\n\\n5/7\",\n",
       "    \" Thanks again to everyone for your awesome Connections\\n\\nAnd congratulations to the winners! You'll see these winning connections soon in the Streamlit docs.  \\n\\n7/7\"]},\n",
       "  'https://x.com/putu_waw/status/1253887166217728006': {'caption': 'Jalan-jalannya dari rumah aja.\\n#StayAtHome',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1248838039314427905': {'caption': 'Quarantine Day 26 = Savage.\\n#MobileLegends\\n#StayAtHome',\n",
       "   'comments': []},\n",
       "  'https://x.com/jokowi/status/1242396278106902529': {'caption': 'Demi mencegah penyebaran Covid-19, para siswa telah \"belajar di rumah\". Di antara mereka, 8,3 juta siswa seharusnya mengikuti ujian nasional dari 106.000 satuan pendidikan di Tanah Air. \\n\\nKarena itulah, pemerintah memutuskan untuk meniadakan ujian nasional (UN) tahun 2020',\n",
       "   'comments': ['Fotonya kok mepet2 paaaak ',\n",
       "    'Bagaimana dengan ujian skripsi pak ? Apakah di tiadakan juga',\n",
       "    'Pakde, mohon untuk meniadakan skripsi juga untuk tahun ini, saya udah beberapa hari ini dirumah saja dan tidak melanjutkan penelitian karena wabah virus corona ini yang makin merajarela\\n\\nDari saya,\\nMahasiswa',\n",
       "    'Pak, yang poto ama bapak anak SMA mana pak? Mukanya seumuran bapak saya semua ',\n",
       "    'Keputusan bijak sih pak',\n",
       "    'SKRIPSI JUGAA PAK SEKALIAN HALOO PAK HALOO MOHON DI DENGAR SUARA HATI SAYA INI',\n",
       "    'Kali ini saya setuju bgt pak presiden.. Lanjut dengan keringanan bayar cicilan kredit..',\n",
       "    'Pak gak sekalian skripsi ditiadakan juga nih ? ',\n",
       "    'makasih pak jokowi',\n",
       "    'Bagus deh, toh UN juga buat pamer-pameran nilai sama temen doang, pak. Masa belajar 3 atau 6 tahun ditentukan 3 atau 4 hari aja ',\n",
       "    'Keju mozzarella khas malangnya kak',\n",
       "    'Kasian yng udah putus karna alesan UN  pak',\n",
       "    'Kapan skripsi di tiadakan kasian angkatan 16 :(',\n",
       "    'Skripsi, tesis, desertasi juga pak hehe',\n",
       "    'Ditiadakan seterusnya, Pak, kalau bisa ',\n",
       "    'Pak tolong dong skripsi di tiadakan, kasian kami mahasiswa semester akhir, udh bosan kami di kampus, kalau skripsi di hapuskan kami janji deh gak bakalan bilang otw lagi padahal baru bangun tidur.',\n",
       "    'SKRIPSI DITIADAKAN JUGA PAK. Penelitian saya lapangan, wawancara, bnyak yg nolak.\\n\\nKan bisa online. Oiya ya.',\n",
       "    'Sidang skripsi juga â€˜ditiadakanâ€™ dong pak, plis ieu mah :(',\n",
       "    'KacungTetapKacung\\nPlongaPlongoPENDUSTA\\n',\n",
       "    'Assalamualaikum pak @jokowi kami mhsiswa semester akhir  memohon utk meniadakan ujian skripsi, penelitian, revisi! Soalnya kami sdh sangat parno utk keluar rmh pak, kampus jg diliburkan uang SPP juga harus kami bayar. Kami tdk sanggup pak! #lulustanpaskripsi #skripsiditiadakan',\n",
       "    'Pak, gimana dgn mahasiswa semester akhir baik S1, S2 maupun S3, udah bayar SPP satu semester, gak sedikit lagi pak, tapi tertunda gak bisa penelitian,gak bisa bimbingan, gak bisa sidang\\nTolong ada kebijakan ya pak',\n",
       "    'Pak bagaimana yang sedang menjalani skripsi ? Ngambil data ngajar ke smp ? Dan sekolah liburnya juga di perpanjang ? Saya sudah ganti materi 3x loh pak  jadi harus ngerubah lagi data awal dan harus bikin instrumen yang baru ',\n",
       "    'Kami tunggu info selanjutnya untuk mahasiswa pak',\n",
       "    'Sekalian tiadakan orang yang dateng pas butuh doang pak',\n",
       "    'Pak ...',\n",
       "    'pak tolong dong UKK untuk anak SMK juga ditiadakan pakUKK kan juga kaya UNBK mengumpulkan massa yang banyak',\n",
       "    'Pak tolong tiadakan juga org ketiga di dunia pakk, saya ga pengen pas sayang dia nya ngilang diambil orang.',\n",
       "    'Oke Pak Presiden mulai saat ini tidak ada alasan\\n\\n\"Aku mau fokus UN dulu\"',\n",
       "    'Pak pak tolong pak buat yg lagi menjalani skripsi juga pak, dilulusin aja napa\\nMau ke perpus kaga bisa, mau riset pun susah, bimbingan pun kudu online yg dimana itu juga susah ',\n",
       "    'tuku sayap, asiyapp:(',\n",
       "    'Apakah ukk smk juga ditiadakan pak?',\n",
       "    'Skripsi jangan dihapus juga pak! Soalnya saya udah ujian, biar mereka merasakan juga apa yang saya rasakan. :v ',\n",
       "    'Pak Jokowi mohon untuk tidak memperpanjang libur untuk anak SMP ,karena semenjak saya liburan saya tidak dapat uang jajan pak',\n",
       "    'TIDAK SESUAI DENGAN SILA KELIMA',\n",
       "    '@AraChan221 REBAHAN REBAHAN HEHE',\n",
       "    'Harus ada pembatasan jam kerja kantor pak',\n",
       "    'Alumni 2020 Lulus Berkat Virus Corona ',\n",
       "    'pak skripsi jg dong plislah duh',\n",
       "    'yaALLAH pak Alhamdulillah, lulus jalur corona ',\n",
       "    'pak coba UN di tiadakan, tp suruh bikin skripsi atau disertasi mungkin hehe',\n",
       "    'pak, gamau mutualan sama saya?',\n",
       "    'Pak tolong pak tolong banget, kalo bikin keputusan lockdown kuota internet digratiskan pak biar kami betah di rumah, apalagi kami kaum rebahan. Itu sangat membantu Lo pak ',\n",
       "    'Pak saya mau penilitian skripsi bagaimana ini pak diluar banyak virus bertebaran pak sedang di lockdown jg kan... Terus kalo bimbingan onlie sinyal dirumah saya kentang pak gaada wifi',\n",
       "    'Dulu rezim jokowi meremeh kan masalah penyebaran virus corona skrang jokowi malah kocar kacir menghadapi penyebaran virus corona TKA asal cina di beri kebebasan masuk ke indonesia tanpa pencengkalan dari pihak kantor imigrasi bandara #jokowigagaltanganicovid19',\n",
       "    '#JokowiMundurSaja',\n",
       "    'Untuk mengurangi penyebaran virus corona pemerintah malaysia telah mendeportasi TKA asal indonesia sedang kan pemerintah indonesia tak berani mendeportasi TKA asal cina yg menyebar kan virus corona di indonesia #JKWJongosChina',\n",
       "    'Virus corona itu bukan di tular kan oleh angin udara atau binatang penyebaran virus corona di indonesia di tular kan oleh warga negara asing yg bebas masuk ke indonesia jokowi harus bertanggung jawab atas kelalaian mencegah penyebaran virus corona di indonesia',\n",
       "    'Hantavirus di Cina:\\nPria, yg terinfeksi virus yg disebabkan tikus, sdg dalam perjalanan ke Provinsi Shandong dng bus ketika dia meninggal.\\n\\nSekitar 32 orang lainnya berada di bus yg sama yg skrg sedang diuji virusnya.\\n\\n #Hantavirus\\nhttps://cnbctv18.com/healthcare/hantavirus-in-china-after-coronavirus-havoc-man-dies-of-rat-caused-disease-all-you-need-to-know-5548201.htmâ€¦',\n",
       "    'Assalamualaikum,selamat pagimaaf mengganggu waktunya. Disini saya wafiq Azizah choirizaldi selaku siswi yang berada di Lampung. Mohon izin berbicara,bagaimana kebijakan jika kemendikbud di provinsi Lampung tetap mengadakan UNBK.',\n",
       "    'mohon di beri keringanan untuk mahasiswa semester akhir pak. karena fakultas fkip penelitian berada di sekolah sedangkan sekolahnya sendiri libur pak gimana coba pak belum lagi bimbingan skripsi yang online luama dan banyak kurang puasnya mohon pak di tindak lanjuti',\n",
       "    'Gabisa mikir nnti setelah ini bakal gimana , mau nyari kuliah mau bulan ramadhan terus tetep ada covidâ€™19 yaallah lekas membaik bumiku',\n",
       "    'Skripsi juga batalin dong pak, capek saya mau penelitian di sekolah gabisa, sekolahnya libur gatau sampai kapan. Trus kapan saya lulus kuliah klo gini ceritanya:(',\n",
       "    'Mereka ga akan ngrasain nikmatnya hembusan pertama pas bar keluar ruang ujian saat hari terakhir, sama nikmatnya beli kunci un dan zonkkkk kuncine ra tembus',\n",
       "    'Azam right now cc @yennyciptian',\n",
       "    '@AyuraYunika @adzilazen Nah ini bkn hoax :v',\n",
       "    'Pak tolong kalo UN ditiadakan, skripsi juga di tiadakan pak. kami sebagai mahasiswa tingkat akhir yg penelitian di lab kampus merasa takut dengan kondisi Indonesia sekarang',\n",
       "    'nih sedikit oleh oleh buat lo dari china dongok \\n\\nhttps://x.com/ChinaDaily/status/1001255579157979136?s=19â€¦',\n",
       "    'Jika UNBK ditiadakan gimana nasib kita yang udah bayar mahal mahal tapi terbuang sia sia pak? Mungkin ada orang diluar sana rela hutang kesana kemari untuk bayar ujian dan ujungnya tidak ada hasilnya? Setidaknya di kembalikan setengahnya kami sudah berterima kasih:) @jokowi',\n",
       "    'skripsi jg dong pak, ih bhaya tau pak bimbingan klo scara lgsungg:)) krna klo bmbingan online ga efektif pak sriuss dahh hhhhe3x',\n",
       "    'Pak, Tugas Akhir / Skripsi kaga mau sekalian pak?',\n",
       "    'Skripsi an di tiada in gak pak? Di tahun ini Aja? Ganti Apa proposal Aja ya :(( please :\"',\n",
       "    'Bagus lah',\n",
       "    'Mantap pak.karena kondisi sekarang sudah memberikan libur di rumah atau online dari rumah',\n",
       "    'tolong pak, dosen saya gaptek.. terus revisi skripsian saya gmna? secara online pun gakbisa , ditelfon juga gabisa. di sms gak bales. ditemuin jg gamau... terus skripsi saya gmna ya pak tolong ',\n",
       "    'Apakah skripsi juga ditiadakan ???',\n",
       "    'Solali lali\\nOla ola laa\\nSMA hepiii\\nSMK oraaa',\n",
       "    'Bagaimana dengan Penundaan Cicilan Ke Bank atau Leasing kendaraan atau Rumah, Sepertinya Pihak BANK atau Leasing belum Memberitahukan kepada Nasabah atau Debitur ?',\n",
       "    'ujian skripsi sekalian, pak',\n",
       "    'Pak skripsi nggak sekalian ditiadain juga pak? Bingung nihh pak mau penelitian kagak bisa juga pak',\n",
       "    'Alhamdulillah,smoga uk strusnya. Ckup pola ujian spt jmn dlu sj. Buktinya yg jd PRSIDEN PRTAMA SP KE BPK PRESIDEN JOKOWI TRCINTA,Jg tdk ada yg ikut ujian nas,sdh trbukti bs jd org2 hebat bhkan Presiden. Sy jg skloh hax d kpg bs juara umum trus,bs msuk SMA fav,bs jd PNS llus MURNI',\n",
       "    'Yang mahasiswa bikin tagar saja #tiadakanskripsi',\n",
       "    'Pak skripsinya ga sekalian ya? Kami ngambil data juga kan ketemu sama orang yang bukan 1 atau 2 orang:( bimbingan juga online pak, ga efektif:(',\n",
       "    'Kalau ujian praktek nya gimana smk saya belum. Tapi kalau un nya sudah selasai',\n",
       "    '@b4kt3riii',\n",
       "    'HariÂ² semakin banyak Korban mati dan korban tertular...sudahlah pak presiden LOCK DOWN INDONESIA atau daerah merah sekarang juga...cobalah gunakan nurani anda, terimalah saran\" tim pakar IDI untuk membendung wabah ini..#LockdownNow #lockdownindonesia',\n",
       "    'Ujian komprehensif utk mahasiswa akhir juga dong pak ',\n",
       "    'Pak adain aplikasi di hp buat alarm Covid - 19 agar membantu yang terjangkit langsung di tangani oleh dokter dari pemerintah jadi nggak ngerasain di tolak sana sini (di tolak di bidan lah dan di rumah sakit kecil gth intiny)',\n",
       "    'Yess gak ada ya putus dengan alasan mau fokus UN',\n",
       "    'Pak tolong kami para medis\\nYang seharusnya Masker sekali pakai dan sekarang tidak lagi, 1 masker untuk 1 minggutisu saja yg diganti2 namun maskernya pemakaian untuk seminggu#curahanHatiMedis',\n",
       "    'Sidang skripsi,sidang KP,sidang DPR yg isinya cuma orang molor semua jg ditiadakan pak,\\nTolong',\n",
       "    'Kita kuliah online kualitas belajar menurun tapi uang semster kaga turun kwkwkwk',\n",
       "    'Kalau skripsi jangan dihapus ya, Pak. Biarkan ia tetap ada.',\n",
       "    'Kasian para cewe gada alasan buat bilang ke cowoknya â€œ aku mau konsen ke UN ,kita udahan ya â€œ',\n",
       "    'Aku doain semoga skripsi menyusul ditiadakan. Mau ora? @71Dimas',\n",
       "    'Yey, gajadi diputusin garaÂ² mau fokus UN dong pak',\n",
       "    'PAK SKRIPSI JUGA DONG. GIMANA MAU PENELITIAN DAH???',\n",
       "    'Terimakasih pak @jokowi dan para menteri atas kebijakannya.. semoga bisa menjadikan kami lebih baik dan sadar akan keadaan saat ini.. semoga dampak virus ini segera hilang dan semua kegiatan kembali seperti biasa... Kami turut prihatin',\n",
       "    'Semangat pak jokowi, saya tau bapak berjuang keras demi rakyat Indonesia meskipun mungkin masih banyak keluhan dan kekurangan sana sini, selalu sehat njeh pak',\n",
       "    'wah kok gk pas jaman biyen ae iki @ariessamudraw',\n",
       "    'Alumni 2020 Lulus jalur Virus Corona ',\n",
       "    'Pak skripsi juga ditiadakan dong pak hehe',\n",
       "    'ga ada gitu pak, mahasiswa akhir yg lg penelitian dan mengurus urusan dikampus utk seminar ditangguhkan aja di smt depan tanpa bayar spp. pusink ini bos mau penelitian ke lapangan temu 90an sampel blm lg ngurus suratnya kampus tutup malah bentar lg smt baru n lebaran. hadeuh',\n",
       "    'Nasib mahasiswa gimana pak? Apalagi bagi mereka yang tidak bisa bimbingan secara online, waktu 14hari yg seharusnya dipake buat bimbingan eh malah harus #dirumahaja gegara corona, sidang dan wisuda mereka jadi terhambat',\n",
       "    'Untuk skripsi bagaimana pa @jokowi ? Terutama bagi yang sedang melakukan penelitian di sekolah sekarang ? Masa iya harus wisuda taun deoan dan gagal cumlaude pa ',\n",
       "    'Berita baik tentang corona hari ini pak. Semoga ini semua benar',\n",
       "    'tugas online yg diberikan dosen juga dibatalkan pak',\n",
       "    'Pak bagaimana nasib mhs yg mengambil skripsi dan objek penelitiannya peserta didik SMA sedangkan anak SMAnya libur  baru kali libur tapi saya sedih',\n",
       "    'Buat yang udh beli buku fokus un sama ikutan les buat un,  anda mantap sekali.',\n",
       "    'Lucky you lil brother.',\n",
       "    'sidang skripsi kapan ditiadakan? ',\n",
       "    'pak mohon yang sedang mengerjakarkan tugas akhir/skripsi bagaimana ya? apakah ditiadakan atau diganti dengan sidang onlen? kalau begini nanti mahasiswa tingkat akhir kapan lulusnya pak ',\n",
       "    '@dangharbor kaga un lu ye ena bnr wkwkwk',\n",
       "    'Pak skripsi paaaak',\n",
       "    'Pa skripsi juga pa. Kita-kita ga bisa ambil data ini ',\n",
       "    'Skripsi juga pak tolong di tiadakan, atau seminar dan sindangnya di tiadakan jdi hanya ngumpulin skripsi nya aja pakkk, orng tua telah mendesak agar cept wisuda pak.',\n",
       "    'Mohon sekiranya Bapak bersedia untuk membebaskan biaya kuota internet, agar anak sekolah bisa belajar scr online di Rumah,.. Suwun,..',\n",
       "    'trus apa pengganti UN???',\n",
       "    'Ada lo @mao_lewat',\n",
       "    'Belajar di rumah lebih efektif jika TV Nasional dan swasta mengganti acara sinetron, film, olahraga, game, musik, berita2 seleb, dan acara2 humoris dg video2 yg berisi materi pendidikan SD-SLTA, minimal 3-4 jam se hari. Stasiun TV jangan nyari untung aja.\\n#KitaDirumahAjaYa',\n",
       "    'Itu mah sudah harus dmk\\nBukan sebuah prestasi ',\n",
       "    'gantiin uang bimbel pak',\n",
       "    'Bapak baik banget deh, bikin terharu ',\n",
       "    'DetikÂ²,bimbel,simulasi?\\nApa kabar?',\n",
       "    'Skripsi gimana pak',\n",
       "    'Lockdown jok, sdh banyak daerah yg zona merah. Tegal sdh berani lockdown, knp @DKIJakarta tdk boleh lockdown?',\n",
       "    'Gimna ya? Kya ada rasa kecewa udh dri dulu tradisinya seklh klo mau luls ujian dulu lah ini g ada ujian padlh bru aja megng komptr. Tpi y udhlah hrusnya sneng g ribet lagi mikirin ujian',\n",
       "    'Pak sidang di tiadakan dong pak, gapapa deh nyusun tugas akhir tp gausa sidang atau ga sidangnya perkelompok atau ga bole di wakilin gtu saya sidang ngambil stnk bole diwakilin masa sidang yg ini gaboleh',\n",
       "    'Skripsi hapus dong pak, saya janji pak, kalau skripsi ditiadakan saya gak main mobile legend lagi',\n",
       "    'Ada yg mau beli buku detik2 ga gann masih mulus no minus dijual karna ga jadi UN :v',\n",
       "    'Kasian chika yang nolak badrun kemaren bikin alasan mau focus UN pak',\n",
       "    'Ini respon murid saya Pak \\nhttps://x.com/rrurr15/status/1242376399375069185?s=19â€¦',\n",
       "    'Tadi sempet buat polling di ige sesaat setelah viedo Mas Menteri beredar dan ini respon anak-anak saya ',\n",
       "    'Hmm',\n",
       "    'Luhut Akui Pemerintah Lamban Tangani Virus Corona\\nhttp://gelora.co/2020/03/luhut-akui-pemerintah-lamban-tangani.htmlâ€¦',\n",
       "    'tolong tiadakan tugas belajar di rumah pak,pusing saya pak tiap hari tugas numpuk deadline beda beda bahaya pak bisa depresi\\njadi tolong tiadakan tugas selama belajar di rumah karena corona ini pak,saya mohon banget ini mah sama bapak demi mengurangi beban hidup kami para pelajar',\n",
       "    'Pak mahasiswa semester akhir apa kabar? Kami tidak bisa melanjutkan penelitian Pak. Sekolah dimana-mana libur semua, kampus juga libur. Kasi kejelasanlah Pak.',\n",
       "    'baca komen2nya mewakili angkatan 16 banget wkwkwk',\n",
       "    'Apakah ada kebijakan pemerintah.....potongan bayar SPP yang mulia? Utk anak sekolah yg diliburkan? Terutama swasta.',\n",
       "    'Ujian kompetensi perawat juga kalau bisa di tindakan pak, jangan di undur juli, biar kita para perawat muda ini bisa langsung kerja pak',\n",
       "    'pemerintah indonesia kurang cepat dan sangat lambat dalam menanggani masalah virus corona (covid19) memang berat ujian untuk president @jokowi sudah seharusnya pemerintah bergerak cepat dalam membasmi covid 19 di indonesia agar keadaan beransur baik dan dapat beraktivitas kembali',\n",
       "    'Yang dulu buka akses wisata selebar-lebarnya dan meremehkan ancaman virus sebelum merebak siapa? \\nBakal dituntut banyak orang. Di dunia dan akhirat.',\n",
       "    'Bapak anak kuliah bayar UKT tidak pak? @jokowi',\n",
       "    'Sebelum pemilihan presiden, rajin bagi-bagi bingkisan, sekarang rakyat butuh masker pak, gak ada bantuan dari bapak.....',\n",
       "    'Waaah...kadung bimbel bayar larang, pak.\\nGak sido kemaki yen UN ditiadakan.\\n',\n",
       "    'Pak? Skripsian gimana?\\nYg penelitianya disekolah? Kapan sekolah bisa efektif? Atau kapan skripsi di batalin juga? Atau mala dihapusin? Pak?',\n",
       "    'Ga sekalian pak skripsi ditiadakan? Saya gabisa turun lapangan nihhh huhuu, kapan wisuda kalo kek gini ',\n",
       "    'Skarang mah ga pake UN juga udh pasti lulus ',\n",
       "    'Aku contoh nya yang mau unbk. Ada seneng nya ada sedih nya . Seneng nya kita bisa libur sedih nya udah terlanjur beli buku sukses unbk 2020 :(',\n",
       "    'Kang @BadruddinEmce @emYazzlubnahl @BANG_ISKHAQ kiye lah tembe info A1 wkwkkwk',\n",
       "    'Skripsi gimana pak?',\n",
       "    'Skripsi pripun pak? :(',\n",
       "    'kasian yg udh diputusin dgn alasan nyuruh fokus un pak',\n",
       "    'Bagaimana dengan skripsi pak ? Di tiadakan juga ',\n",
       "    'kasian yang udah putus karena alesan UN pak',\n",
       "    'Kepada adekÂ² siswa/i angkatan 2020.. Selamat anda lulus jalur corona.',\n",
       "    'Trs bagaimana dg kelulusan pak,  apa wisuda juga di tiadakan?',\n",
       "    'Negara tetangga .. menghimbau jika pe duduk masih bepergian jika terkena sakit karna corona biaya rumah sakit disuruh nanggung sendiri Full ... well done #KitaDirumahAjaYa',\n",
       "    'Wah gaswat~ Bisnis bocoran soal UN onlineku bisa gulung tikar ini... ',\n",
       "    'Izin pak kalau utbk gmn ya pak?saya udah gapyear selama setahun semenjak lulus sma biar bisa lulus ptn,pak utbksbmptn 2020 ini gk bisa diringankan kaya unbk?dijadiin bebas tes masuk ptn gt pak?ya allah bahagia dunia akhirat pak seriusannotice pak plis,sendingbuat bapak',\n",
       "    'Di rumah aja nnti kita Mabar',\n",
       "    'UKK untuk SMK 2020 gimana?',\n",
       "    'Sebelum masuk indonesia itu harusnya di cegah.jngn cuma mikirin ekonomi. Kalo udah begini bukan cuma ekonomi.tapi rakyat pada mati',\n",
       "    'tolong untuk tenaga pengajar nya dihimbau juga pak, supaya memberi tugas sewajarnya dan deadline yg masuk akal',\n",
       "    'Dengan penuh hormat pak.\\n\\nTolong juga di tiadakan skripsian, anggota dpr, dan temen yg nongkrong padahal punya rokok tapi ngisep rokok orang.\\n\\nTrims pak. -jajang sarung di cikapundung',\n",
       "    'Dari pada bayar design grafis, mending pakek canva',\n",
       "    'adek saya nangis pak malah gajadi UN gara2 udh niat dan bayar les pula',\n",
       "    'Pak tolong lah tugas tugas juga di tiadakan',\n",
       "    'Kalau terkait dengan Skripsi masih seperti biasakan pak ?',\n",
       "    'Mohon pak presiden sosialisasikan   agar uang steril dari virus covid 19. terimakasih',\n",
       "    \"Kita belajar dari china tentang pencegahan covid 19' bagaimana cara agar uang yg digunakan dimasyarakat steril dari covid 19. Terimakasih\",\n",
       "    'enjoyyyyyy',\n",
       "    'Dilarang berkumpul pak atau mengumpulkan massa.. itu fotonya di ganti sm foto bpk sendiri aja ',\n",
       "    'Lulus jalur corona',\n",
       "    'SKRIPSI SEKALIAN DONG PAK, BIAR ADIL ',\n",
       "    'Mantap pak Jokowi',\n",
       "    'Alhamdulillah. ',\n",
       "    'Boong pak, ga pada belajar di rumah juga',\n",
       "    'Bapa harus menerapkan kebijakan sementara lock down termasuk antar daerah demi nyawa rakyat bapa...',\n",
       "    'Tp jgn kasih pe er banyak2 pak utk anak2 dirumah, soalnya ortu jg WFH jd nambah byk kerjaan mana dikasih waktu utk ngerjain dari gurunya. Jd pada rebutan utk diajarin pelajarannya..',\n",
       "    'Ukom ngga sekalian juga pak?\\nKasian kan yg pengen kerja ngga bs karna harus terhalang ukom diundur, diundurnya lama bgt :\"',\n",
       "    '#RezimJKWPembohong',\n",
       "    \"Assalamu'alaikum bapak presidenku tercinta. Apa tidak mau meniadakan SKRIPSI, KTI, beserta SIDANG-SIDANG nya pak. Mohon dengarkan curahan hati mahasiswa tingkat akhir yang entah nasib nya bagaimana karena kebanyakan sidang belom juga penelitian praktek pak. Suwun njih pak \",\n",
       "    'Tugas mahasiswa gak sekaliak pak?? Gak kena corona malah kena depresi garaÂ² tugas :(',\n",
       "    'Keju mozarella khas malangnya Pak',\n",
       "    'Alhamdulillah pak @jokowi semua ada hikmah dibalik musibah',\n",
       "    'Alhamdulillah']},\n",
       "  'https://x.com/putu_waw/status/1220496942519554048': {'caption': 'Mari bingungkan anak zaman sekarang.\\n#platypus',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1216726296220225536': {'caption': ' : \"Aku itu orangnya kadang suka kayak larutan yang mampu mempertahankan pH.\"\\n : \"Apaan tuh?\"\\n : \"Buffer :v\"\\n#Chemistry',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1212040819391123457': {'caption': '1 / 366 - Leap Year\\n#happynewyear2020',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1208591386821320704': {'caption': 'Koramil 1603 Seririt.',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1205885544732868608': {'caption': 'Sedih sih kalau begini ceritanya\\n@FiersaBesari\\n#SoundofJustice',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1159307146783154176': {'caption': 'Training of Trainer (ToT)\\n\"Optimalisasi Penyediaan Akses Internet untuk Mendukung Sektor Pendidikan dan Pariwisata\".\\n@bahaso',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1151243345110294528': {'caption': 'Lunar Eclipse \\n#GerhanaBulanSebagian\\n#EclipseLunar',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1143386205066629121': {'caption': ' : \"Can we take a picture?\"\\n : \"Of course, gather at mid.\"\\n : \"Take english class!\"\\n\\n#MobileLegends\\n#LagiMales',\n",
       "   'comments': ['Nice pick wkwkkwkwkw']},\n",
       "  'https://x.com/putu_waw/status/1140272844858777602': {'caption': 'Terimakasih telah menjadi ayah kedua kami.\\nâ€¢  Wayan Suarsa, S.Pd. â€¢',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1137016268580503553': {'caption': 'X MIA 1 - Vedicience.',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1133278655046492161': {'caption': 'Kalau aku sambat, memangnya kenapa? @nksthi',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1118875128782180353': {'caption': 'Inget masa-masa dimana liat hubungan orang cuma dari status.  #GoodbyeBBM',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1111973639123959808': {'caption': 'Bersatu Merangkai Warna Nusantara | HUT 415 Singaraja',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1056487916334067713': {'caption': '[Late Post]\\nSalam dari Gianyar untuk Smanser. #SMANSERFIRE',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1041716765611286528': {'caption': 'Serah terima jabatan kepengurusan OSIS SMA Negeri 1 Seririt Masa Bhakti 2017/2018 ke OSIS SMA Negeri 1 Seririt Masa Bhakti 2018/2019. Good luck!',\n",
       "   'comments': []}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.get(\"https://www.facebook.com/putu.widyantara.3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_xpath_from_bs4_element(element: PageElement) -> Optional[str]:\n",
    "    try:\n",
    "        components = []\n",
    "        while element:\n",
    "            siblings = element.find_previous_siblings(element.name)\n",
    "            if siblings:  # only add index if there are siblings\n",
    "                index = len(siblings) + 1\n",
    "                components.append(f\"{element.name}[{index}]\")\n",
    "            else:\n",
    "                components.append(f\"{element.name}\")\n",
    "            element = element.parent\n",
    "        result = \"/\" + \"/\".join(reversed(components))\n",
    "        result = result.replace(\"/[document]\", \"\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fb_post_id(total_post: int = -1) -> List[str]:\n",
    "    result = set()\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        spans = soup.find_all(\n",
    "            \"span\",\n",
    "            class_=\"x4k7w5x x1h91t0o x1h9r5lt x1jfb8zj xv2umb2 x1beo9mf xaigb6o x12ejxvf x3igimt xarpa2k xedcshv x1lytzrv x1t2pt76 x7ja8zs x1qrby5j\",\n",
    "        )\n",
    "        for span in spans:\n",
    "            a_tags = span.find_all(\n",
    "                \"a\",\n",
    "                class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1sur9pj xkrqix3 xi81zsa x1s688f\",\n",
    "            )\n",
    "            for a_tag in a_tags:\n",
    "                if \"href\" in a_tag.attrs:\n",
    "                    post_id: str = a_tag[\"href\"]\n",
    "                    if post_id.startswith(\"https\"):\n",
    "                        params_idx = post_id.find(\"?\")\n",
    "                        if params_idx != -1:\n",
    "                            result.add(post_id[:params_idx])\n",
    "                        else:\n",
    "                            result.add(post_id)\n",
    "                    else:\n",
    "                        # href not converted into post id\n",
    "                        # need to hover on the link to make it change\n",
    "                        logger.warning(\"Found href not converted into post id\")\n",
    "                        try:\n",
    "                            xpath = _get_xpath_from_bs4_element(a_tag)\n",
    "                            element = webdriver.find_element(By.XPATH, xpath)\n",
    "                            action = ActionChains(webdriver)\n",
    "                            action.move_to_element(element).perform()\n",
    "                        except Exception as e:\n",
    "                            logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "        history.append(len(result))\n",
    "        if total_post != -1 and len(result) >= total_post:\n",
    "            break\n",
    "\n",
    "        logger.info(f\"Total post id scraped: {len(result)}\")\n",
    "        if len(history) > 5:\n",
    "            if history[-5] == history[-1]:\n",
    "                logger.info(\"No new post found\")\n",
    "                break\n",
    "\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption():\n",
    "    caption = \"\"\n",
    "    new_soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    outer_divs = new_soup.find_all(\n",
    "        \"div\",\n",
    "        class_=\"x1l90r2v x1pi30zi x1swvt13 x1iorvi4\",\n",
    "        attrs={\"data-ad-preview\": \"message\"},\n",
    "    )\n",
    "    for d in outer_divs:\n",
    "        new_divs = d.find_all(\"div\", class_=\"xu06os2 x1ok221b\")\n",
    "        for div in new_divs:\n",
    "            span = div.find_all(\n",
    "                \"span\",\n",
    "                class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\",\n",
    "            )\n",
    "            for s in span:\n",
    "                caption += s.text\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments():\n",
    "    result = list()\n",
    "    history = list()\n",
    "    \n",
    "    repeat = True\n",
    "    while repeat:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "\n",
    "        # click all replied\n",
    "        try:\n",
    "            replied_buttons = webdriver.find_elements(By.XPATH, \"//span[contains(text(), 'replied')]\")\n",
    "            for element in replied_buttons:\n",
    "                element.click()\n",
    "\n",
    "            more_comments_button = webdriver.find_elements(By.XPATH, \"//span[contains(text(), 'more comments')]\")\n",
    "            for element in more_comments_button:\n",
    "                element.click()\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"xwib8y2 xn6708d x1ye3gou x1y1aw1k\")\n",
    "\n",
    "        history.append(len(divs))\n",
    "        logger.info(f\"Searching more comments, found: {len(divs)}\")\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new comments found\")\n",
    "                break\n",
    "\n",
    "    logger.info(\"Start scrapping comments\")\n",
    "    soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    spans = soup.find_all(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u\")\n",
    "    for span in spans:\n",
    "        divs = span.find_all(\"div\", class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\")\n",
    "        for div in divs:\n",
    "            anchors = div.find_all(\"a\")\n",
    "            replied_username = None\n",
    "            for a in anchors:\n",
    "                if a:\n",
    "                    replied_username = a.text\n",
    "            comment: str = (\" \".join(div.stripped_strings))\n",
    "            if replied_username and comment.startswith(replied_username):\n",
    "                comment = comment[len(replied_username)+1:] # +1 to remove space\n",
    "                result.append(comment)\n",
    "            else:\n",
    "                result.append(comment)\n",
    "    logger.info(f\"Total comments scraped: {len(result)}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_facebook(user_id, num_post=-1):\n",
    "    dataset = Dataset()\n",
    "\n",
    "    webdriver.get(f\"{FACEBOOK_BASE_URL}/{user_id}\")\n",
    "    list_post_id = get_fb_post_id(total_post=num_post)\n",
    "    for url in list_post_id:\n",
    "        webdriver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        caption = get_caption()\n",
    "        comments = get_comments()\n",
    "        post = Post(caption=caption, comments=comments)\n",
    "        dataset.data.data.update({url: post})\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-20 23:55:08.803] [WARNING] Found href not converted into post id\n",
      "[2024-10-20 23:55:09.348] [INFO] Total post id scraped: 1\n",
      "[2024-10-20 23:55:10.245] [WARNING] Found href not converted into post id\n",
      "[2024-10-20 23:55:10.568] [WARNING] Found href not converted into post id\n",
      "[2024-10-20 23:55:10.896] [INFO] Total post id scraped: 2\n",
      "[2024-10-20 23:55:11.694] [WARNING] Found href not converted into post id\n",
      "[2024-10-20 23:55:12.032] [INFO] Total post id scraped: 4\n",
      "[2024-10-20 23:55:12.829] [WARNING] Found href not converted into post id\n",
      "[2024-10-20 23:55:13.129] [INFO] Total post id scraped: 4\n",
      "[2024-10-20 23:55:13.880] [INFO] Total post id scraped: 5\n",
      "[2024-10-20 23:55:14.828] [INFO] Total post id scraped: 5\n",
      "[2024-10-20 23:55:15.634] [INFO] Total post id scraped: 5\n",
      "[2024-10-20 23:55:16.394] [INFO] Total post id scraped: 5\n",
      "[2024-10-20 23:55:17.178] [INFO] Total post id scraped: 5\n",
      "[2024-10-20 23:55:17.178] [INFO] No new post found\n",
      "[2024-10-20 23:55:25.627] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:26.260] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:26.894] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:27.528] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:28.194] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:28.826] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:29.426] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:30.051] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:30.643] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:31.261] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:32.026] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:32.042] [INFO] No new comments found\n",
      "[2024-10-20 23:55:32.042] [INFO] Start scrapping comments\n",
      "[2024-10-20 23:55:32.343] [INFO] Total comments scraped: 0\n",
      "[2024-10-20 23:55:37.961] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:38.641] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:39.274] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:39.866] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:40.493] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:41.109] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:41.744] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:42.374] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:42.961] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:43.574] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:44.216] [INFO] Searching more comments, found: 0\n",
      "[2024-10-20 23:55:44.217] [INFO] No new comments found\n",
      "[2024-10-20 23:55:44.217] [INFO] Start scrapping comments\n",
      "[2024-10-20 23:55:44.657] [INFO] Total comments scraped: 0\n",
      "[2024-10-20 23:55:51.109] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:55:51.841] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:52.473] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:53.189] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:53.839] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:54.506] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:55.170] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:55.906] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:56.509] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:57.122] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:57.704] [INFO] Searching more comments, found: 6\n",
      "[2024-10-20 23:55:57.705] [INFO] No new comments found\n",
      "[2024-10-20 23:55:57.705] [INFO] Start scrapping comments\n",
      "[2024-10-20 23:55:57.955] [INFO] Total comments scraped: 6\n",
      "[2024-10-20 23:56:04.854] [INFO] Searching more comments, found: 2\n",
      "[2024-10-20 23:56:05.538] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:06.389] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:07.038] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:07.637] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:08.254] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:08.853] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:09.504] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:10.120] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:10.723] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:11.396] [INFO] Searching more comments, found: 5\n",
      "[2024-10-20 23:56:11.396] [INFO] No new comments found\n",
      "[2024-10-20 23:56:11.396] [INFO] Start scrapping comments\n",
      "[2024-10-20 23:56:11.854] [INFO] Total comments scraped: 5\n",
      "[2024-10-20 23:56:19.603] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:20.252] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:20.888] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:21.552] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:22.169] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:22.819] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:23.471] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:24.133] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:24.785] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:25.581] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:26.167] [INFO] Searching more comments, found: 1\n",
      "[2024-10-20 23:56:26.167] [INFO] No new comments found\n",
      "[2024-10-20 23:56:26.167] [INFO] Start scrapping comments\n",
      "[2024-10-20 23:56:26.466] [INFO] Total comments scraped: 1\n"
     ]
    }
   ],
   "source": [
    "facebook_dataset = scraping_facebook(\"putu.widyantara.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://web.facebook.com/putu.widyantara.3/posts/pfbid02AhYqduykDBeeLKLK2otwBurPxQbWgGQqHmPpEp8vScqi9ywAocL5YUFApvSpBnrTl': {'caption': 'Melepas rasa penat setelah UAS 1... Traveling to Lovina',\n",
       "   'comments': []},\n",
       "  'https://web.facebook.com/itenpradya/posts/pfbid016FYevTzAwiHNEhY7NqgpSUe18PPppDu9A265yJgiVw9ndpurF4zYk5UpaxtRY58l': {'caption': '121',\n",
       "   'comments': []},\n",
       "  'https://web.facebook.com/putu.widyantara.3/posts/pfbid037in6a5686XMT5PviGPudSFigbqkHstwnHEuYgkpS41dXG82AuBvKaLYsUMHRtGCYl': {'caption': '[Late Post]Serah terima jabatan kepengurusan OSIS SMA Negeri 1 Seririt Masa Bhakti 2017/2018 ke OSIS SMA Negeri 1 Seririt Masa Bhakti 2018/2019. Good luck!',\n",
       "   'comments': ['Adikku mn kok gk klhtn yah hehe',\n",
       "    'Ada kok Bu Herlina Wati , no 8 dari kanan',\n",
       "    'bes cenik2 sing tpuk',\n",
       "    'Pt jadi osis y...',\n",
       "    'Ndak Om Artana Putu , itu dokumentasi dr pelantikan OSIS masa bhakti 2018/2019, nnti klo di tahunnya Putu 2019/2020.',\n",
       "    'yy...mudah2n nti trpilih jdi osis...ikuti j kgiatn2 osis...']},\n",
       "  'https://web.facebook.com/tuti.andayani/posts/pfbid02gqZpopGRh9PC2zYzGoawSqEShkp1nkpYTpa7ETwRJ1n8U3WsVvnjnbLm9zNvBdhAl': {'caption': 'Selamat bulan bahasa Tetap solid tim #ksptiksmanser',\n",
       "   'comments': ['Asik Kayaknya bukan cuma pascal aja sekarang I Wayan Widiastina',\n",
       "    'hhmmm sudah banyak berubah sepertinya yaa',\n",
       "    'Hahahaha hai senior Rama Danuartha I Wayan Widiastina apa kbrnya?  Trimakasih dedikasi bwt smanser Pascal tetap dihati tapi perkembangan zaman tetap ikuti',\n",
       "    'Waauuu kerenn tik',\n",
       "    'pang rame den']},\n",
       "  'https://web.facebook.com/putu.widyantara.3/posts/pfbid02pTKcFcAnwrBEvGth8QRQPrKy7gmvxw5P7EyxFnioR1NEbqmM34JjfDLN7FYQj6VAl': {'caption': 'Rahajeng Nyepi Tahun Baru Caka 1939. Ngiring ngelaksanayang Catur Brata penyepian.',\n",
       "   'comments': ['']}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa'}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset = Dataset.from_json(\"dataset.json\")\n",
    "current_dataset.data.data.update(facebook_dataset.data.data)\n",
    "current_dataset.to_json(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get caption and comments for FB reels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dataset = Dataset.from_json(\"dataset.json\")\n",
    "# current_dataset.data.data.update(instagram_dataset.data.data)\n",
    "# current_dataset.data.data.update(twitter_dataset.data.data)\n",
    "# current_dataset.to_json(\"dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
