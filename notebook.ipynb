{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import PageElement\n",
    "from pydantic import BaseModel\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.remote.webelement import WebElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-25 12:41:06.913] [INFO] Logger setup complete\n"
     ]
    }
   ],
   "source": [
    "class CustomFormatter(logging.Formatter):\n",
    "    log_format = \"[%(asctime)s.%(msecs)03d] [%(levelname)s] %(message)s\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(self.log_format, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# remove any existing handlers to prevent double logging\n",
    "if logging.getLogger().hasHandlers():\n",
    "    logging.getLogger().handlers.clear()\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(CustomFormatter())\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "def log_func(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logger.info(f\"[{func.__name__}] args: {args}, kwargs: {kwargs}\")\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "logger.info(\"Logger setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(BaseModel):\n",
    "    caption: Optional[str]  # Caption can be None or a string\n",
    "    comments: Optional[List[Optional[str]]]  # Comments can be None or a list of strings\n",
    "\n",
    "\n",
    "class DatasetModel(BaseModel):\n",
    "    data: Dict[str, Post]  # URL keys with Post values\n",
    "    author: Optional[str] = (\n",
    "        \"Putu Widyantara Artanta Wibawa\"\n",
    "    )\n",
    "    updated_at: Optional[str] = None\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data_dict=None):\n",
    "        \"\"\"Initialize with a dictionary and store it in self.data after validation.\"\"\"\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        self.data = DatasetModel(data=data_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_file):\n",
    "        \"\"\"Load JSON file, validate it, and return a new Dataset instance.\"\"\"\n",
    "        try:\n",
    "            with open(json_file, \"r\") as file:\n",
    "                json_data = json.load(file)\n",
    "                return cls(data_dict=json_data.get(\"data\", {}))\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: {json_file} not found.\")\n",
    "            return cls()\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"Error: Could not decode JSON from {json_file}.\")\n",
    "            return cls()\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Validation error: {e}\")\n",
    "            return cls()\n",
    "\n",
    "    def to_json(self, json_file):\n",
    "        \"\"\"Save self.data to a JSON file.\"\"\"\n",
    "        with open(json_file, \"w\") as file:\n",
    "            self.data.updated_at = datetime.now().isoformat()\n",
    "            json.dump(self.data.model_dump(), file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_BASE_URL = \"https://x.com\"\n",
    "INSTAGRAM_BASE_URL = \"https://www.instagram.com\"\n",
    "FACEBOOK_BASE_URL = \"https://www.facebook.com\"\n",
    "TIKTOK_BASE_URL = \"https://www.tiktok.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-18 12:08:22.757] [WARNING] Error sending stats to Plausible: error sending request for url (https://plausible.io/api/event)\n"
     ]
    }
   ],
   "source": [
    "webdriver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.get(INSTAGRAM_BASE_URL)\n",
    "\n",
    "# need login first, so wait for user to login\n",
    "# time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def show_first_post_ig(url: str):\n",
    "    try:\n",
    "        webdriver.get(url)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\n",
    "            \"div\",\n",
    "            class_=\"x1lliihq x1n2onr6 xh8yej3 x4gyw5p xfllauq xo2y696 x11i5rnm x2pgyrj\",\n",
    "        )\n",
    "        list_urls = []\n",
    "\n",
    "        for div in divs:\n",
    "            a_tag = div.find(\"a\", recursive=False)\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                list_urls.append(a_tag[\"href\"])\n",
    "\n",
    "        element = webdriver.find_element(By.XPATH, f'//a[@href=\"{list_urls[0]}\"]')\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_caption_ig() -> Optional[str]:\n",
    "    try:\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"_a9zs\")\n",
    "        for div in divs:\n",
    "            h1_tag = div.find(\n",
    "                \"h1\", class_=\"_ap3a _aaco _aacu _aacx _aad7 _aade\", recursive=False\n",
    "            )\n",
    "            for br in h1_tag.find_all(\"br\"):\n",
    "                br.replace_with(\"\\n\")\n",
    "            if h1_tag:\n",
    "                return h1_tag.text\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_element_xpath(element: WebElement) -> Optional[str]:\n",
    "    try:\n",
    "        full_xpath = webdriver.execute_script(\n",
    "            \"\"\"\n",
    "            function getElementXPath(element) {\n",
    "                if (element.id !== '') {\n",
    "                    return 'id(\"' + element.id + '\")';\n",
    "                }\n",
    "                if (element === document.body) {\n",
    "                    return element.tagName.toLowerCase();\n",
    "                }\n",
    "\n",
    "                let ix = 0;\n",
    "                const siblings = element.parentNode.childNodes;\n",
    "                let sameTagSiblings = 0;\n",
    "\n",
    "                for (let i = 0; i < siblings.length; i++) {\n",
    "                    if (siblings[i].nodeType === 1 && siblings[i].tagName === element.tagName) {\n",
    "                        sameTagSiblings++;\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                for (let i = 0; i < siblings.length; i++) {\n",
    "                    const sibling = siblings[i];\n",
    "                    if (sibling === element) {\n",
    "                        let text = \"\";\n",
    "                        \n",
    "                        if (sameTagSiblings > 1) {\n",
    "                            text = '[' + (ix + 1) + ']';\n",
    "                        }\n",
    "                        \n",
    "                        return getElementXPath(element.parentNode) + '/' + element.tagName.toLowerCase() + text;\n",
    "                    }\n",
    "\n",
    "                    if (sibling.nodeType === 1 && sibling.tagName === element.tagName) {\n",
    "                        ix++;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return getElementXPath(arguments[0]);\n",
    "\n",
    "        \"\"\",\n",
    "            element,\n",
    "        )\n",
    "        result = f\"/html/{full_xpath}\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def load_more_comments_ig():\n",
    "    try:\n",
    "        title = webdriver.find_element(\n",
    "            By.XPATH, \"//*[contains(text(), 'Load more comments')]\"\n",
    "        )\n",
    "        if title:\n",
    "            is_found = True\n",
    "            while is_found:\n",
    "                try:\n",
    "                    title = webdriver.find_element(\n",
    "                        By.XPATH, \"//*[contains(text(), 'Load more comments')]\"\n",
    "                    )\n",
    "                    title_xpath = _get_element_xpath(title)\n",
    "                    button_xpath = title_xpath[\n",
    "                        : title_xpath.rfind(\"button\") + len(\"button\")\n",
    "                    ]\n",
    "                    try:\n",
    "                        button_element = webdriver.find_element(By.XPATH, button_xpath)\n",
    "                        button_element.click()\n",
    "                    except Exception as e:\n",
    "                        logger.error(str(e).split(\"\\n\")[0])\n",
    "                        is_found = False\n",
    "                except Exception as e:\n",
    "                    logger.error(str(e).split(\"\\n\")[0])\n",
    "                    is_found = False\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def show_replies_ig():\n",
    "    try:\n",
    "\n",
    "        button = webdriver.find_elements(\n",
    "            \"xpath\", \"//button[contains(@class, '_acan _acao _acas _aj1- _ap30')]\"\n",
    "        )\n",
    "        result_button = [\n",
    "            b\n",
    "            for b in button\n",
    "            if (b.text.startswith(\"View replies\") or b.text.startswith(\"View all\"))\n",
    "        ]\n",
    "        total_button = len(result_button)\n",
    "        if total_button > 0:\n",
    "            for b in result_button:\n",
    "                b.click()\n",
    "            logger.info(f\"Total button clicked: {total_button}\")\n",
    "        else:\n",
    "            logger.warning(\"No replies found\")\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_comments_ig() -> list[str]:\n",
    "    try:\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        comments = soup.find_all(\"div\", class_=\"_a9zs\")\n",
    "        result = []\n",
    "        for div in comments:\n",
    "            span_tag = div.find(\n",
    "                \"span\", class_=\"_ap3a _aaco _aacu _aacx _aad7 _aade\", recursive=False\n",
    "            )\n",
    "            if span_tag:\n",
    "                result.append(span_tag.text)\n",
    "        logger.info(f\"Total comments found: {len(result)}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def next_post_ig():\n",
    "    try:\n",
    "        button = webdriver.find_element(\n",
    "            By.XPATH,\n",
    "            f'//span[@style=\"display: inline-block; transform: rotate(90deg);\"]',\n",
    "        )\n",
    "        button.click()\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "\n",
    "@log_func\n",
    "def has_next_post_ig() -> bool:\n",
    "    try:\n",
    "        webdriver.find_element(\n",
    "            By.XPATH,\n",
    "            f'//span[@style=\"display: inline-block; transform: rotate(90deg);\"]',\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _get_single_post_data_ig() -> Post:\n",
    "    load_more_comments_ig()\n",
    "    show_replies_ig()\n",
    "    caption = get_caption_ig()\n",
    "    comments = get_comments_ig()\n",
    "    return Post(caption=caption, comments=comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_instagram(username: str, max_posts: Optional[int] = -1) -> Dataset:\n",
    "    try:\n",
    "        if max_posts == 0:\n",
    "            return Dataset()\n",
    "\n",
    "        result = Dataset()\n",
    "        url = f\"{INSTAGRAM_BASE_URL}/{username}/\"\n",
    "        show_first_post_ig(url)\n",
    "\n",
    "        # get data\n",
    "        post_data = _get_single_post_data_ig()\n",
    "        # 'https://www.instagram.com/p/:POST_ID/?img_index=1'\n",
    "        post_id = webdriver.current_url\n",
    "        result.data.data.update({post_id: post_data})\n",
    "\n",
    "        max_posts -= 1 # because we already get the first post\n",
    "        if max_posts == -1:\n",
    "            while has_next_post_ig():\n",
    "                next_post_ig()\n",
    "                time.sleep(2)\n",
    "                post_data = _get_single_post_data_ig()\n",
    "                post_id = webdriver.current_url\n",
    "                result.data.data.update({post_id: post_data})\n",
    "        else:\n",
    "            while max_posts and has_next_post_ig():\n",
    "                next_post_ig()\n",
    "                max_posts -= 1\n",
    "                time.sleep(2)\n",
    "                post_data = _get_single_post_data_ig()\n",
    "                post_id = webdriver.current_url\n",
    "                result.data.data.update({post_id: post_data})\n",
    "            if max_posts:\n",
    "                logger.warning(\"Total post less than expected\")\n",
    "\n",
    "        # stats\n",
    "        scraped_posts = len(result.data.data)\n",
    "        scraped_comments = sum(\n",
    "            len(post.comments) for post in result.data.data.values()\n",
    "        )\n",
    "        logger.info(f\"Total post scraped: {scraped_posts}\")\n",
    "        logger.info(f\"Total comments scraped: {scraped_comments}\")\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-17 22:06:13.642] [INFO] [show_first_post]\n",
      "[2024-10-17 22:06:17.310] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:17.312] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:17.339] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:17.340] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:17.384] [WARNING] No replies found\n",
      "[2024-10-17 22:06:17.385] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:17.900] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:18.430] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:18.439] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:18.453] [INFO] [next_post]\n",
      "[2024-10-17 22:06:20.585] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:20.586] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:20.604] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:20.605] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:20.704] [INFO] Total button clicked: 1\n",
      "[2024-10-17 22:06:20.704] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:21.105] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:21.550] [INFO] Total comments found: 4\n",
      "[2024-10-17 22:06:21.562] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:21.594] [INFO] [next_post]\n",
      "[2024-10-17 22:06:23.757] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:23.758] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:23.781] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:23.783] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:23.845] [WARNING] No replies found\n",
      "[2024-10-17 22:06:23.846] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:24.299] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:24.592] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:24.599] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:24.606] [INFO] [next_post]\n",
      "[2024-10-17 22:06:26.706] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:26.707] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:26.728] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:26.729] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:26.775] [WARNING] No replies found\n",
      "[2024-10-17 22:06:26.776] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:27.188] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:27.612] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:27.622] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:27.633] [INFO] [next_post]\n",
      "[2024-10-17 22:06:29.734] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:29.734] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:29.753] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:29.754] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:29.826] [INFO] Total button clicked: 1\n",
      "[2024-10-17 22:06:29.827] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:30.286] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:30.666] [INFO] Total comments found: 3\n",
      "[2024-10-17 22:06:30.672] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:30.681] [INFO] [next_post]\n",
      "[2024-10-17 22:06:32.775] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:32.777] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:32.980] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:32.981] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:33.128] [WARNING] No replies found\n",
      "[2024-10-17 22:06:33.129] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:33.815] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:34.490] [INFO] Total comments found: 24\n",
      "[2024-10-17 22:06:34.500] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:34.513] [INFO] [next_post]\n",
      "[2024-10-17 22:06:36.721] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:36.722] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:36.739] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:36.739] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:36.780] [WARNING] No replies found\n",
      "[2024-10-17 22:06:36.781] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:37.463] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:37.928] [INFO] Total comments found: 0\n",
      "[2024-10-17 22:06:37.937] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:37.948] [INFO] [next_post]\n",
      "[2024-10-17 22:06:40.106] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:40.106] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:40.125] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:40.126] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:40.138] [WARNING] No replies found\n",
      "[2024-10-17 22:06:40.139] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:40.740] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:41.328] [INFO] Total comments found: 4\n",
      "[2024-10-17 22:06:41.339] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:41.352] [INFO] [next_post]\n",
      "[2024-10-17 22:06:43.500] [INFO] [_get_single_post_data]\n",
      "[2024-10-17 22:06:43.500] [INFO] [load_more_comments]\n",
      "[2024-10-17 22:06:43.523] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[contains(text(), 'Load more comments')]\"}\n",
      "[2024-10-17 22:06:43.524] [INFO] [show_replies]\n",
      "[2024-10-17 22:06:43.534] [WARNING] No replies found\n",
      "[2024-10-17 22:06:43.535] [INFO] [get_caption]\n",
      "[2024-10-17 22:06:44.081] [INFO] [get_comments]\n",
      "[2024-10-17 22:06:44.603] [INFO] Total comments found: 1\n",
      "[2024-10-17 22:06:44.647] [INFO] [has_next_post]\n",
      "[2024-10-17 22:06:44.663] [ERROR] Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//span[@style=\"display: inline-block; transform: rotate(90deg);\"]\"}\n",
      "[2024-10-17 22:06:44.664] [INFO] Total post scraped: 9\n",
      "[2024-10-17 22:06:44.664] [INFO] Total comments scraped: 36\n"
     ]
    }
   ],
   "source": [
    "instagram_dataset = scraping_instagram(\"putu_waw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://www.instagram.com/p/C3W-SslrQpp/': {'caption': 'Halo seluruh mahasiswa Indonesia. Saya siap mengikuti Magang dan Studi Independen Bersertifikat Angkatan 6!',\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/Cv4iiXPLDl6/': {'caption': 'Halo! Saya Putu Widyantara Artanta Wibawa dari Universitas Udayana siap mengikuti National Onboarding MSIB Angkatan 5!\\n\\n#BerprosesLebihBaik #KampusMerdeka #MSIB5 #MagangMerdeka #MagangBersertifikat #BukanMagangdanStudiBiasa #MSIB5',\n",
       "   'comments': ['Mangaaat', '🔥', 'Great My son😍', 'Semangat frenn🔥']},\n",
       "  'https://www.instagram.com/p/CZqhWPWlNYN/': {'caption': '[SAYA SIAP MENGIKUTI MAHASISYA UPANAYANA XIX]\\n\\nOm Swastyastu 🙏\\n\"Om Ano Bhadrah Kratavo Yantu Visvatah\" - (Yajur Veda XXV. 14)\\n(Semoga pikiran yang baik datang dari segala penjuru)\\n\\nMahasisya Upanayana merupakan upacara penyucian diri dengan tujuan memohon doa restu secara niskala tatkala seorang mahasiswa akan menuntut ilmu dan berguru di Universitas Udayana.\\n\\nSaya Putu Widyantara Artanta Wibawa, Siap mengikuti Mahasisya Upanayana XIX tahun 2022. \\n\\n\"Tad viddhi praņipātena\\nParipraśneņa sēvayā\\nUpadekşyanti te jñānam\\nJñāninas tattva darśinah\"\\n(Bhagavadgita IV. 34)\\n\\n(Kejarlah kebijakan itu dengan kerendahan hati, dengan bertanya-tanya dan dengan pengabdian. Orang bijaksana yang melihat kebenaran itu akan memberi petunjuk padamu tentang pengetahuan itu).\\n\\n🙏🏻 Om Santih, Santih, Santih Om 🙏🏻\\nSatyam Eva Jayate!',\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/CStykH6lYRU/': {'caption': \"[I'M READY FOR STUDENT DAY FMIPA 2021]\\n\\nHalo, sobat MIPA!👋\\nPerkenalkan saya Putu Widyantara Artanta Wibawa, mahasiswa Program Studi Informatika siap mengikuti Student Day FMIPA 2021. Sampai jumpa di Hari Puncak Student Day FMIPA 2021 pada tanggal 20 Agustus 2021.\\n\\nSaya MIPA, Saya Bangga!\\n\\n@bemfmipaunud @fmipaunud\\n#StudentDayFMIPAUnud2021\\n#MIPAJaya\\n#ProudToBeSilver\",\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/CSgDdyaLOuL/': {'caption': '[PKKMB FMIPA]\\n\\nSaya Putu Widyantara Artanta Wibawa, dari Fakultas Matematika dan Ilmu Pengetahuan Alam, Prodi Informatika.\\n\\nSaya siap mengikuti PKKMB FMIPA pada tanggal 18-19 Agustus 2021.\\n\\n@fmipaunud\\n#fmipaUnud\\n#FakultasMIPAUniversitasUdayana\\n#PKKMBFMIPA2021\\n#PKKMBUNUD2021\\n#PKKMB2021',\n",
       "   'comments': ['Kepak sayap FMIPA🔥',\n",
       "    'Jurusan apa kau? Semangat yooo',\n",
       "    '@prabhageg ngambil jurusan ilmu komputer. Siapp, thanks prabha🙌']},\n",
       "  'https://www.instagram.com/p/CSbwvz_lyUK/': {'caption': '[Saya Ksatria Muda Udayana Siap Mengikuti Student Day 2021]\\n\\nJalan-jalan ke Kota Tua\\nDi Kota Tua ada Car-Free Day\\nMohon izin kakak-kakak semua\\nSaya siap ikuti Student Day\\n\\nSaya Putu Widyantara Artanta Wibawa siap mengikuti Student Day 2021 Universitas Udayana pada tanggal 12-13 Agustus 2021.\\n\\n#StudentDay2021Unud\\n#ParwataArundaya',\n",
       "   'comments': ['Semangat putu🔥🔥',\n",
       "    'mangadd kak putuww🔥',\n",
       "    'Semangat cokkk🔥',\n",
       "    'keren tuu🔥',\n",
       "    'mangatt putu!🔥🙌',\n",
       "    'Semangatt putuu🔥🔥',\n",
       "    'mangat cak caknya🔥🔥',\n",
       "    'Semangat weee🙌🔥',\n",
       "    'Mangats waww',\n",
       "    'Mangattt waw🔥🔥',\n",
       "    'Inget kiri-kanan Tuu🙏🏼🤝🏻',\n",
       "    'Ow semangat 🔥',\n",
       "    'Semangat bang 🔥',\n",
       "    'semangaatt putuu🙌🔥',\n",
       "    'MANGAT WAW🔥',\n",
       "    'Mangattt putuuu!🙌',\n",
       "    'Semangat putuu 🔥',\n",
       "    'MANGATTT PUTU WAWWWW🔥🔥',\n",
       "    'semangat tuu',\n",
       "    'Mangaaattsss',\n",
       "    'Semangat tu🔥',\n",
       "    'semangattt broo 🤩',\n",
       "    '🔥🔥🔥',\n",
       "    'semangatt waw!']},\n",
       "  'https://www.instagram.com/p/B4Uyx6FlxyM/': {'caption': 'TPF20192410_007_Putu Widyantara Artanta Wibawa_Sore-Sore Ring Pelabuhan Buleleng_Objek Wisata di Bali\\n\\nSore-sore ring pelabuhan Buleleng\\nPemandangan yukti lintang asri\\nOmbak ane menepi pesisi\\nRikala Sang Surya jagi pineleb.\\nKutipan lagu karya Gede Darma ini menjadi saksi keindahan Eks Pelabuhan Buleleng yang pernah menjadi dermaga terbesar di Pulau Bali sebelum pusat pemerintah Provinsi Bali dipindah ke Bali Selatan.\\n\\nDi kawasan Eks Pelabuhan Buleleng dibangun monumen Yudha Mandala untuk mengenang perjuangan rakyat Bali disaat berjuang melawan  penjajahan Belanda. \\nKayu-kayu bekas yang berumur tua di dermaga ini dirubah menjadi restoran terapung yang memiliki desain yang unik dengan panorama pantai yang berpadu dengan deburan ombak serta semilir angin yang berhembus lembut menerpa tubuh.\\n\\nInilah Eks Pelabuhan Buleleng yang sekarang, dengan sajian keindahannya yang begitu memesona dan sarat akan nilai sejarah. [Keterangan Teknis]\\nDevice : OPPO F7\\nFocal Length : 3.62 mm\\nAperture : f/1.8\\nISO : 40\\nFlash : No flash\\nWhite Balance : Auto\\nExposure Time : 1/374 s\\n[Tags]\\n@odowkun_ @pringga_arts @rosyanaputra_ @hima_tp_undiksha #tpfotografi2019 #himatpundiksha',\n",
       "   'comments': []},\n",
       "  'https://www.instagram.com/p/BRhg04YjO0w/': {'caption': 'Landscape Photography by : @putu_waw. This photo took on Sunday, 12 March 2017 at 06.20 (GMT + 8).',\n",
       "   'comments': ['So nicee',\n",
       "    '#travel',\n",
       "    'Pretty cool!',\n",
       "    'GREAT TRAVEL...Like it a lot 🤗.']},\n",
       "  'https://www.instagram.com/p/BPwIVQzBJ6t/': {'caption': 'Landscape Photography by : @putu_waw. This photo took on Wednesday, 14 December 2016 at 05 : 53 (GMT + 8).',\n",
       "   'comments': ['👌']}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa'}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagram_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dataset = Dataset.from_json(\"aneh.json\")\n",
    "# current_dataset.data.data.update(instagram_dataset.data.data)\n",
    "# current_dataset.to_json(\"aneh.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.get(TWITTER_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _scraping_profile_tweet(dataset: Dataset, post: Optional[int] = -1):\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "\n",
    "        url_list = []\n",
    "        div_elements = soup.find_all(\"div\", class_=\"css-175oi2r r-18u37iz r-1q142lx\")\n",
    "        for div_element in div_elements:\n",
    "            a_tag = div_element.find(\"a\", recursive=False)\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                url_list.append(a_tag[\"href\"])\n",
    "\n",
    "        min_idx = min(len(divs), len(url_list))\n",
    "        for idx in range(min_idx):\n",
    "            url = f\"{TWITTER_BASE_URL}{url_list[idx]}\"\n",
    "            dataset.data.data.update(\n",
    "                {url: Post(caption=divs[idx].text, comments=[])}\n",
    "            )\n",
    "\n",
    "        length_data = len(dataset.data.data)\n",
    "        logger.info(f\"Total tweets scraped: {length_data}\")\n",
    "        if post != -1 and length_data >= post:\n",
    "            break\n",
    "        history.append(length_data)\n",
    "\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new tweets found\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _scraping_tweet_comment():\n",
    "    result = list()\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"})\n",
    "\n",
    "        for div in divs:\n",
    "            if div.text not in result:\n",
    "                result.append(div.text)\n",
    "\n",
    "        logger.info(f\"Total comment tweets scraped: {len((result))}\")\n",
    "        history.append(len(result))\n",
    "\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new comment found\")\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_twitter(username: str, max_posts: Optional[int] = -1) -> Dataset:\n",
    "    dataset = Dataset()\n",
    "    webdriver.get(f\"{TWITTER_BASE_URL}/{username}\")\n",
    "    _scraping_profile_tweet(dataset, max_posts)\n",
    "\n",
    "    for url in dataset.data.data.keys():\n",
    "        webdriver.get(f\"{url}\")\n",
    "        logger.info(f\"Scraping comments for tweet: {url}\")\n",
    "        time.sleep(4)\n",
    "\n",
    "        comments = _scraping_tweet_comment()\n",
    "\n",
    "        current_caption = dataset.data.data.get(url).caption\n",
    "        comments.remove(current_caption)\n",
    "\n",
    "        logger.info(f\"Final comments scraped: {len(comments)}\")\n",
    "        dataset.data.data.get(url).comments = comments\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-17 22:19:59.310] [INFO] [_scraping_profile_tweet]\n",
      "[2024-10-17 22:19:59.887] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:00.298] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:00.677] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:01.057] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:01.579] [INFO] Total tweets scraped: 0\n",
      "[2024-10-17 22:20:02.445] [INFO] Total tweets scraped: 3\n",
      "[2024-10-17 22:20:02.957] [INFO] Total tweets scraped: 3\n",
      "[2024-10-17 22:20:03.372] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:03.753] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:04.146] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:04.577] [INFO] Total tweets scraped: 4\n",
      "[2024-10-17 22:20:05.024] [INFO] Total tweets scraped: 5\n",
      "[2024-10-17 22:20:05.493] [INFO] Total tweets scraped: 6\n",
      "[2024-10-17 22:20:05.899] [INFO] Total tweets scraped: 6\n",
      "[2024-10-17 22:20:06.389] [INFO] Total tweets scraped: 7\n",
      "[2024-10-17 22:20:06.869] [INFO] Total tweets scraped: 7\n",
      "[2024-10-17 22:20:07.337] [INFO] Total tweets scraped: 8\n",
      "[2024-10-17 22:20:07.732] [INFO] Total tweets scraped: 8\n",
      "[2024-10-17 22:20:08.141] [INFO] Total tweets scraped: 9\n",
      "[2024-10-17 22:20:08.593] [INFO] Total tweets scraped: 11\n",
      "[2024-10-17 22:20:09.081] [INFO] Total tweets scraped: 12\n",
      "[2024-10-17 22:20:09.518] [INFO] Total tweets scraped: 12\n",
      "[2024-10-17 22:20:10.005] [INFO] Total tweets scraped: 13\n",
      "[2024-10-17 22:20:10.529] [INFO] Total tweets scraped: 13\n",
      "[2024-10-17 22:20:10.944] [INFO] Total tweets scraped: 14\n",
      "[2024-10-17 22:20:11.432] [INFO] Total tweets scraped: 15\n",
      "[2024-10-17 22:20:11.919] [INFO] Total tweets scraped: 15\n",
      "[2024-10-17 22:20:12.377] [INFO] Total tweets scraped: 16\n",
      "[2024-10-17 22:20:12.821] [INFO] Total tweets scraped: 16\n",
      "[2024-10-17 22:20:13.279] [INFO] Total tweets scraped: 17\n",
      "[2024-10-17 22:20:13.711] [INFO] Total tweets scraped: 17\n",
      "[2024-10-17 22:20:14.222] [INFO] Total tweets scraped: 18\n",
      "[2024-10-17 22:20:14.630] [INFO] Total tweets scraped: 18\n",
      "[2024-10-17 22:20:15.057] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:15.452] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:15.850] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:16.249] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:16.648] [INFO] Total tweets scraped: 19\n",
      "[2024-10-17 22:20:17.124] [INFO] Total tweets scraped: 21\n",
      "[2024-10-17 22:20:17.559] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:18.038] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:18.497] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:18.981] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:19.402] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:19.787] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:20.179] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:20.578] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:20.957] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:21.350] [INFO] Total tweets scraped: 22\n",
      "[2024-10-17 22:20:21.351] [INFO] No new tweets found\n",
      "[2024-10-17 22:20:21.656] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1747615071537361226\n",
      "[2024-10-17 22:20:25.658] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:26.017] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:26.364] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:26.717] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:27.077] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:27.510] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:27.859] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:28.212] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:28.567] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:28.922] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:29.275] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:29.633] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:29.634] [INFO] No new comment found\n",
      "[2024-10-17 22:20:29.634] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:20:29.795] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1747614627301867992\n",
      "[2024-10-17 22:20:33.797] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:34.156] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:34.507] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:34.860] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:35.209] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:35.565] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:35.921] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:36.265] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:36.631] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:36.982] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:37.334] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:37.686] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:37.687] [INFO] No new comment found\n",
      "[2024-10-17 22:20:37.687] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:20:37.833] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1747613877423206496\n",
      "[2024-10-17 22:20:41.835] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:42.199] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:42.554] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:42.907] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:43.275] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:43.711] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:44.059] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:44.436] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:44.811] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.161] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.509] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.866] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:45.867] [INFO] No new comment found\n",
      "[2024-10-17 22:20:45.867] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:20:46.010] [INFO] Scraping comments for tweet: https://x.com/streamlit/status/1690045031640375296\n",
      "[2024-10-17 22:20:50.012] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:50.390] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:50.757] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:51.110] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:51.471] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:51.827] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:52.190] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:52.555] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:52.915] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:53.270] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:53.716] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:54.070] [INFO] Total comment tweets scraped: 3\n",
      "[2024-10-17 22:20:54.071] [INFO] No new comment found\n",
      "[2024-10-17 22:20:54.072] [INFO] Final comments scraped: 2\n",
      "[2024-10-17 22:20:54.216] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1253887166217728006\n",
      "[2024-10-17 22:20:58.218] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:20:58.582] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:58.933] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:59.289] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:59.637] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:20:59.987] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:00.347] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:00.696] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:01.040] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:01.384] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:01.729] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:02.074] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:02.074] [INFO] No new comment found\n",
      "[2024-10-17 22:21:02.075] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:21:02.230] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1248838039314427905\n",
      "[2024-10-17 22:21:06.232] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:21:06.597] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:06.948] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:07.304] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:07.667] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:08.024] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:08.395] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:08.743] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:09.091] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:09.526] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:09.873] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:10.221] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:21:10.221] [INFO] No new comment found\n",
      "[2024-10-17 22:21:10.222] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:21:10.363] [INFO] Scraping comments for tweet: https://x.com/jokowi/status/1242396278106902529\n",
      "[2024-10-17 22:21:14.365] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:21:14.758] [INFO] Total comment tweets scraped: 10\n",
      "[2024-10-17 22:21:15.159] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:15.550] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:15.935] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:16.327] [INFO] Total comment tweets scraped: 11\n",
      "[2024-10-17 22:21:17.084] [INFO] Total comment tweets scraped: 20\n",
      "[2024-10-17 22:21:17.649] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:18.089] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:18.534] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:18.978] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:19.409] [INFO] Total comment tweets scraped: 21\n",
      "[2024-10-17 22:21:20.081] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:20.536] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:20.969] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:21.393] [INFO] Total comment tweets scraped: 31\n",
      "[2024-10-17 22:21:22.005] [INFO] Total comment tweets scraped: 41\n",
      "[2024-10-17 22:21:22.464] [INFO] Total comment tweets scraped: 41\n",
      "[2024-10-17 22:21:23.368] [INFO] Total comment tweets scraped: 45\n",
      "[2024-10-17 22:21:24.061] [INFO] Total comment tweets scraped: 46\n",
      "[2024-10-17 22:21:24.534] [INFO] Total comment tweets scraped: 46\n",
      "[2024-10-17 22:21:25.124] [INFO] Total comment tweets scraped: 48\n",
      "[2024-10-17 22:21:25.687] [INFO] Total comment tweets scraped: 48\n",
      "[2024-10-17 22:21:26.145] [INFO] Total comment tweets scraped: 48\n",
      "[2024-10-17 22:21:26.573] [INFO] Total comment tweets scraped: 49\n",
      "[2024-10-17 22:21:27.024] [INFO] Total comment tweets scraped: 50\n",
      "[2024-10-17 22:21:27.420] [INFO] Total comment tweets scraped: 50\n",
      "[2024-10-17 22:21:27.828] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:28.202] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:28.575] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:29.075] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:29.450] [INFO] Total comment tweets scraped: 51\n",
      "[2024-10-17 22:21:30.354] [INFO] Total comment tweets scraped: 60\n",
      "[2024-10-17 22:21:30.929] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:31.449] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:31.972] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:32.387] [INFO] Total comment tweets scraped: 61\n",
      "[2024-10-17 22:21:33.200] [INFO] Total comment tweets scraped: 67\n",
      "[2024-10-17 22:21:33.946] [INFO] Total comment tweets scraped: 70\n",
      "[2024-10-17 22:21:34.429] [INFO] Total comment tweets scraped: 70\n",
      "[2024-10-17 22:21:34.890] [INFO] Total comment tweets scraped: 70\n",
      "[2024-10-17 22:21:35.907] [INFO] Total comment tweets scraped: 80\n",
      "[2024-10-17 22:21:36.532] [INFO] Total comment tweets scraped: 80\n",
      "[2024-10-17 22:21:37.123] [INFO] Total comment tweets scraped: 80\n",
      "[2024-10-17 22:21:37.963] [INFO] Total comment tweets scraped: 85\n",
      "[2024-10-17 22:21:38.938] [INFO] Total comment tweets scraped: 90\n",
      "[2024-10-17 22:21:39.411] [INFO] Total comment tweets scraped: 90\n",
      "[2024-10-17 22:21:39.993] [INFO] Total comment tweets scraped: 90\n",
      "[2024-10-17 22:21:40.993] [INFO] Total comment tweets scraped: 97\n",
      "[2024-10-17 22:21:41.791] [INFO] Total comment tweets scraped: 97\n",
      "[2024-10-17 22:21:42.531] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:43.175] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:43.664] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:44.159] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:44.690] [INFO] Total comment tweets scraped: 100\n",
      "[2024-10-17 22:21:45.956] [INFO] Total comment tweets scraped: 110\n",
      "[2024-10-17 22:21:46.581] [INFO] Total comment tweets scraped: 110\n",
      "[2024-10-17 22:21:47.248] [INFO] Total comment tweets scraped: 110\n",
      "[2024-10-17 22:21:48.224] [INFO] Total comment tweets scraped: 118\n",
      "[2024-10-17 22:21:48.798] [INFO] Total comment tweets scraped: 119\n",
      "[2024-10-17 22:21:49.281] [INFO] Total comment tweets scraped: 119\n",
      "[2024-10-17 22:21:49.857] [INFO] Total comment tweets scraped: 119\n",
      "[2024-10-17 22:21:50.949] [INFO] Total comment tweets scraped: 126\n",
      "[2024-10-17 22:21:51.614] [INFO] Total comment tweets scraped: 128\n",
      "[2024-10-17 22:21:52.346] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:52.863] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:53.461] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:53.941] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:54.369] [INFO] Total comment tweets scraped: 131\n",
      "[2024-10-17 22:21:55.206] [INFO] Total comment tweets scraped: 138\n",
      "[2024-10-17 22:21:55.871] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:56.423] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:56.992] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:57.429] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:57.874] [INFO] Total comment tweets scraped: 141\n",
      "[2024-10-17 22:21:58.620] [INFO] Total comment tweets scraped: 148\n",
      "[2024-10-17 22:21:59.393] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:21:59.943] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:00.402] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:00.839] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:01.260] [INFO] Total comment tweets scraped: 151\n",
      "[2024-10-17 22:22:02.434] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:02.897] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:03.518] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:03.929] [INFO] Total comment tweets scraped: 161\n",
      "[2024-10-17 22:22:04.807] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:05.384] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:05.880] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:06.513] [INFO] Total comment tweets scraped: 171\n",
      "[2024-10-17 22:22:07.289] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:07.748] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:08.207] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:08.632] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:09.063] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:09.575] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:09.978] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:10.400] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:10.806] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:11.205] [INFO] Total comment tweets scraped: 177\n",
      "[2024-10-17 22:22:11.207] [INFO] No new comment found\n",
      "[2024-10-17 22:22:11.207] [INFO] Final comments scraped: 176\n",
      "[2024-10-17 22:22:11.527] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1220496942519554048\n",
      "[2024-10-17 22:22:15.529] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:15.888] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:16.234] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:16.594] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:16.962] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:17.313] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:17.659] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:18.103] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:18.449] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:18.796] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:19.153] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:19.503] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:19.504] [INFO] No new comment found\n",
      "[2024-10-17 22:22:19.504] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:19.647] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1216726296220225536\n",
      "[2024-10-17 22:22:23.648] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:23.996] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:24.354] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:24.705] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:25.052] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:25.402] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:25.747] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:26.099] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:26.450] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:26.805] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:27.149] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:27.496] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:27.497] [INFO] No new comment found\n",
      "[2024-10-17 22:22:27.497] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:27.649] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1212040819391123457\n",
      "[2024-10-17 22:22:31.651] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:32.001] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:32.352] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:32.698] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:33.046] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:33.392] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:33.736] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:34.175] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:34.523] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:34.871] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:35.229] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:35.583] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:35.583] [INFO] No new comment found\n",
      "[2024-10-17 22:22:35.584] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:35.730] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1208591386821320704\n",
      "[2024-10-17 22:22:39.732] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:40.093] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:40.447] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:40.791] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:41.142] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:41.493] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:41.838] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:42.186] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:42.541] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:42.889] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:43.238] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:43.584] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:43.585] [INFO] No new comment found\n",
      "[2024-10-17 22:22:43.586] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:43.724] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1205885544732868608\n",
      "[2024-10-17 22:22:47.726] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:48.089] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:48.460] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:48.808] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:49.168] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:49.516] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:49.980] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:50.340] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:50.707] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.055] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.407] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.753] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:51.753] [INFO] No new comment found\n",
      "[2024-10-17 22:22:51.754] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:51.900] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1159307146783154176\n",
      "[2024-10-17 22:22:55.902] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:22:56.263] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:56.621] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:56.966] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:57.324] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:57.670] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:58.017] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:58.367] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:58.713] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.061] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.409] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.754] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:22:59.754] [INFO] No new comment found\n",
      "[2024-10-17 22:22:59.755] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:22:59.897] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1151243345110294528\n",
      "[2024-10-17 22:23:03.899] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:04.255] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:04.606] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:04.949] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:05.302] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:05.733] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:06.081] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:06.427] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:06.774] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.129] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.477] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.822] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:07.823] [INFO] No new comment found\n",
      "[2024-10-17 22:23:07.823] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:07.965] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1143386205066629121\n",
      "[2024-10-17 22:23:11.966] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:12.328] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:12.691] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:13.052] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:13.399] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:13.757] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:14.109] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:14.459] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:14.817] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.173] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.522] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.875] [INFO] Total comment tweets scraped: 2\n",
      "[2024-10-17 22:23:15.876] [INFO] No new comment found\n",
      "[2024-10-17 22:23:15.876] [INFO] Final comments scraped: 1\n",
      "[2024-10-17 22:23:16.013] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1140272844858777602\n",
      "[2024-10-17 22:23:20.015] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:20.368] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:20.727] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:21.159] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:21.518] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:21.871] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:22.214] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:22.609] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:22.962] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.310] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.652] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.997] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:23.997] [INFO] No new comment found\n",
      "[2024-10-17 22:23:23.998] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:24.137] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1137016268580503553\n",
      "[2024-10-17 22:23:28.138] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:28.496] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:28.846] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:29.196] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:29.548] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:29.896] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:30.248] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:30.606] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:30.951] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:31.299] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:31.655] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:32.012] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:32.013] [INFO] No new comment found\n",
      "[2024-10-17 22:23:32.013] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:32.155] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1133278655046492161\n",
      "[2024-10-17 22:23:36.157] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:36.518] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:36.864] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:37.299] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:37.648] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:37.993] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:38.338] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:38.691] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:39.040] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:39.388] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:39.737] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:40.086] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:40.087] [INFO] No new comment found\n",
      "[2024-10-17 22:23:40.088] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:40.234] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1118875128782180353\n",
      "[2024-10-17 22:23:44.236] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:44.589] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:44.939] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:45.287] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:45.630] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:45.977] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:46.324] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:46.667] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:47.015] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:47.367] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:47.718] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:48.067] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:48.068] [INFO] No new comment found\n",
      "[2024-10-17 22:23:48.068] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:48.215] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1111973639123959808\n",
      "[2024-10-17 22:23:52.216] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:23:52.574] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:52.937] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:53.375] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:53.725] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:54.082] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:54.436] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:54.779] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:55.141] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:55.502] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:55.861] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:56.210] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:23:56.211] [INFO] No new comment found\n",
      "[2024-10-17 22:23:56.212] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:23:56.355] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1056487916334067713\n",
      "[2024-10-17 22:24:00.356] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:24:00.722] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:01.091] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:01.433] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:01.781] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:02.135] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:02.482] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:02.826] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:03.172] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:03.514] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:03.862] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:04.206] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:04.207] [INFO] No new comment found\n",
      "[2024-10-17 22:24:04.208] [INFO] Final comments scraped: 0\n",
      "[2024-10-17 22:24:04.353] [INFO] Scraping comments for tweet: https://x.com/putu_waw/status/1041716765611286528\n",
      "[2024-10-17 22:24:08.355] [INFO] [_scraping_tweet_comment]\n",
      "[2024-10-17 22:24:08.717] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:09.160] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:09.504] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:09.851] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:10.197] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:10.544] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:10.896] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:11.288] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:11.640] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:11.990] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:12.332] [INFO] Total comment tweets scraped: 1\n",
      "[2024-10-17 22:24:12.333] [INFO] No new comment found\n",
      "[2024-10-17 22:24:12.333] [INFO] Final comments scraped: 0\n"
     ]
    }
   ],
   "source": [
    "twitter_dataset = scraping_twitter(\"putu_waw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://x.com/putu_waw/status/1747615071537361226': {'caption': \"The waiting is over! I'm very happy because I have already get the final transcript for Bangkit 2023. I hope I can become one of the Bangkit distinct graduation \\n#lifeatbangkit\",\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1747614627301867992': {'caption': 'Finally, I completed 2 optional courses given by Banfkit about TensorFlow Advance Technique and NLP. Next is completing the Dicoding course hehe\\n\\n#lifeatbangkit',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1747613877423206496': {'caption': \"Hi everyone, I'm very happy to share with you that finally I completed all of the course at Bangkit. Letsgoo\\n#lifeatbangkit\",\n",
       "   'comments': []},\n",
       "  'https://x.com/streamlit/status/1690045031640375296': {'caption': \" Putu Widyantara Artanta Wibawa \\n\\n@putu_waw's @CockroachDB Connection! The demo app shows how to build the connection and query the database.\\n\\n Connection: https://buff.ly/4412DWs\\n App: https://buff.ly/4499NYt \\n\\n6/7\",\n",
       "   'comments': [\" Pedro Toledo \\n\\n@pedrotol_'s @trychroma Connection! The demo gives examples on how to use Chroma on two of the three available deployment modes, and instructions on how to test the third.\\n\\n Connection: https://buff.ly/444bypO\\n App: https://buff.ly/3YJp69z \\n\\n5/7\",\n",
       "    \" Thanks again to everyone for your awesome Connections\\n\\nAnd congratulations to the winners! You'll see these winning connections soon in the Streamlit docs.  \\n\\n7/7\"]},\n",
       "  'https://x.com/putu_waw/status/1253887166217728006': {'caption': 'Jalan-jalannya dari rumah aja.\\n#StayAtHome',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1248838039314427905': {'caption': 'Quarantine Day 26 = Savage.\\n#MobileLegends\\n#StayAtHome',\n",
       "   'comments': []},\n",
       "  'https://x.com/jokowi/status/1242396278106902529': {'caption': 'Demi mencegah penyebaran Covid-19, para siswa telah \"belajar di rumah\". Di antara mereka, 8,3 juta siswa seharusnya mengikuti ujian nasional dari 106.000 satuan pendidikan di Tanah Air. \\n\\nKarena itulah, pemerintah memutuskan untuk meniadakan ujian nasional (UN) tahun 2020',\n",
       "   'comments': ['Fotonya kok mepet2 paaaak ',\n",
       "    'Bagaimana dengan ujian skripsi pak ? Apakah di tiadakan juga',\n",
       "    'Pakde, mohon untuk meniadakan skripsi juga untuk tahun ini, saya udah beberapa hari ini dirumah saja dan tidak melanjutkan penelitian karena wabah virus corona ini yang makin merajarela\\n\\nDari saya,\\nMahasiswa',\n",
       "    'Pak, yang poto ama bapak anak SMA mana pak? Mukanya seumuran bapak saya semua ',\n",
       "    'Keputusan bijak sih pak',\n",
       "    'SKRIPSI JUGAA PAK SEKALIAN HALOO PAK HALOO MOHON DI DENGAR SUARA HATI SAYA INI',\n",
       "    'Kali ini saya setuju bgt pak presiden.. Lanjut dengan keringanan bayar cicilan kredit..',\n",
       "    'Pak gak sekalian skripsi ditiadakan juga nih ? ',\n",
       "    'makasih pak jokowi',\n",
       "    'Bagus deh, toh UN juga buat pamer-pameran nilai sama temen doang, pak. Masa belajar 3 atau 6 tahun ditentukan 3 atau 4 hari aja ',\n",
       "    'Keju mozzarella khas malangnya kak',\n",
       "    'Kasian yng udah putus karna alesan UN  pak',\n",
       "    'Kapan skripsi di tiadakan kasian angkatan 16 :(',\n",
       "    'Skripsi, tesis, desertasi juga pak hehe',\n",
       "    'Ditiadakan seterusnya, Pak, kalau bisa ',\n",
       "    'Pak tolong dong skripsi di tiadakan, kasian kami mahasiswa semester akhir, udh bosan kami di kampus, kalau skripsi di hapuskan kami janji deh gak bakalan bilang otw lagi padahal baru bangun tidur.',\n",
       "    'SKRIPSI DITIADAKAN JUGA PAK. Penelitian saya lapangan, wawancara, bnyak yg nolak.\\n\\nKan bisa online. Oiya ya.',\n",
       "    'Sidang skripsi juga ‘ditiadakan’ dong pak, plis ieu mah :(',\n",
       "    'KacungTetapKacung\\nPlongaPlongoPENDUSTA\\n',\n",
       "    'Assalamualaikum pak @jokowi kami mhsiswa semester akhir  memohon utk meniadakan ujian skripsi, penelitian, revisi! Soalnya kami sdh sangat parno utk keluar rmh pak, kampus jg diliburkan uang SPP juga harus kami bayar. Kami tdk sanggup pak! #lulustanpaskripsi #skripsiditiadakan',\n",
       "    'Pak, gimana dgn mahasiswa semester akhir baik S1, S2 maupun S3, udah bayar SPP satu semester, gak sedikit lagi pak, tapi tertunda gak bisa penelitian,gak bisa bimbingan, gak bisa sidang\\nTolong ada kebijakan ya pak',\n",
       "    'Pak bagaimana yang sedang menjalani skripsi ? Ngambil data ngajar ke smp ? Dan sekolah liburnya juga di perpanjang ? Saya sudah ganti materi 3x loh pak  jadi harus ngerubah lagi data awal dan harus bikin instrumen yang baru ',\n",
       "    'Kami tunggu info selanjutnya untuk mahasiswa pak',\n",
       "    'Sekalian tiadakan orang yang dateng pas butuh doang pak',\n",
       "    'Pak ...',\n",
       "    'pak tolong dong UKK untuk anak SMK juga ditiadakan pakUKK kan juga kaya UNBK mengumpulkan massa yang banyak',\n",
       "    'Pak tolong tiadakan juga org ketiga di dunia pakk, saya ga pengen pas sayang dia nya ngilang diambil orang.',\n",
       "    'Oke Pak Presiden mulai saat ini tidak ada alasan\\n\\n\"Aku mau fokus UN dulu\"',\n",
       "    'Pak pak tolong pak buat yg lagi menjalani skripsi juga pak, dilulusin aja napa\\nMau ke perpus kaga bisa, mau riset pun susah, bimbingan pun kudu online yg dimana itu juga susah ',\n",
       "    'tuku sayap, asiyapp:(',\n",
       "    'Apakah ukk smk juga ditiadakan pak?',\n",
       "    'Skripsi jangan dihapus juga pak! Soalnya saya udah ujian, biar mereka merasakan juga apa yang saya rasakan. :v ',\n",
       "    'Pak Jokowi mohon untuk tidak memperpanjang libur untuk anak SMP ,karena semenjak saya liburan saya tidak dapat uang jajan pak',\n",
       "    'TIDAK SESUAI DENGAN SILA KELIMA',\n",
       "    '@AraChan221 REBAHAN REBAHAN HEHE',\n",
       "    'Harus ada pembatasan jam kerja kantor pak',\n",
       "    'Alumni 2020 Lulus Berkat Virus Corona ',\n",
       "    'pak skripsi jg dong plislah duh',\n",
       "    'yaALLAH pak Alhamdulillah, lulus jalur corona ',\n",
       "    'pak coba UN di tiadakan, tp suruh bikin skripsi atau disertasi mungkin hehe',\n",
       "    'pak, gamau mutualan sama saya?',\n",
       "    'Pak tolong pak tolong banget, kalo bikin keputusan lockdown kuota internet digratiskan pak biar kami betah di rumah, apalagi kami kaum rebahan. Itu sangat membantu Lo pak ',\n",
       "    'Pak saya mau penilitian skripsi bagaimana ini pak diluar banyak virus bertebaran pak sedang di lockdown jg kan... Terus kalo bimbingan onlie sinyal dirumah saya kentang pak gaada wifi',\n",
       "    'Dulu rezim jokowi meremeh kan masalah penyebaran virus corona skrang jokowi malah kocar kacir menghadapi penyebaran virus corona TKA asal cina di beri kebebasan masuk ke indonesia tanpa pencengkalan dari pihak kantor imigrasi bandara #jokowigagaltanganicovid19',\n",
       "    '#JokowiMundurSaja',\n",
       "    'Untuk mengurangi penyebaran virus corona pemerintah malaysia telah mendeportasi TKA asal indonesia sedang kan pemerintah indonesia tak berani mendeportasi TKA asal cina yg menyebar kan virus corona di indonesia #JKWJongosChina',\n",
       "    'Virus corona itu bukan di tular kan oleh angin udara atau binatang penyebaran virus corona di indonesia di tular kan oleh warga negara asing yg bebas masuk ke indonesia jokowi harus bertanggung jawab atas kelalaian mencegah penyebaran virus corona di indonesia',\n",
       "    'Hantavirus di Cina:\\nPria, yg terinfeksi virus yg disebabkan tikus, sdg dalam perjalanan ke Provinsi Shandong dng bus ketika dia meninggal.\\n\\nSekitar 32 orang lainnya berada di bus yg sama yg skrg sedang diuji virusnya.\\n\\n #Hantavirus\\nhttps://cnbctv18.com/healthcare/hantavirus-in-china-after-coronavirus-havoc-man-dies-of-rat-caused-disease-all-you-need-to-know-5548201.htm…',\n",
       "    'Assalamualaikum,selamat pagimaaf mengganggu waktunya. Disini saya wafiq Azizah choirizaldi selaku siswi yang berada di Lampung. Mohon izin berbicara,bagaimana kebijakan jika kemendikbud di provinsi Lampung tetap mengadakan UNBK.',\n",
       "    'mohon di beri keringanan untuk mahasiswa semester akhir pak. karena fakultas fkip penelitian berada di sekolah sedangkan sekolahnya sendiri libur pak gimana coba pak belum lagi bimbingan skripsi yang online luama dan banyak kurang puasnya mohon pak di tindak lanjuti',\n",
       "    'Gabisa mikir nnti setelah ini bakal gimana , mau nyari kuliah mau bulan ramadhan terus tetep ada covid’19 yaallah lekas membaik bumiku',\n",
       "    'Skripsi juga batalin dong pak, capek saya mau penelitian di sekolah gabisa, sekolahnya libur gatau sampai kapan. Trus kapan saya lulus kuliah klo gini ceritanya:(',\n",
       "    'Mereka ga akan ngrasain nikmatnya hembusan pertama pas bar keluar ruang ujian saat hari terakhir, sama nikmatnya beli kunci un dan zonkkkk kuncine ra tembus',\n",
       "    'Azam right now cc @yennyciptian',\n",
       "    '@AyuraYunika @adzilazen Nah ini bkn hoax :v',\n",
       "    'Pak tolong kalo UN ditiadakan, skripsi juga di tiadakan pak. kami sebagai mahasiswa tingkat akhir yg penelitian di lab kampus merasa takut dengan kondisi Indonesia sekarang',\n",
       "    'nih sedikit oleh oleh buat lo dari china dongok \\n\\nhttps://x.com/ChinaDaily/status/1001255579157979136?s=19…',\n",
       "    'Jika UNBK ditiadakan gimana nasib kita yang udah bayar mahal mahal tapi terbuang sia sia pak? Mungkin ada orang diluar sana rela hutang kesana kemari untuk bayar ujian dan ujungnya tidak ada hasilnya? Setidaknya di kembalikan setengahnya kami sudah berterima kasih:) @jokowi',\n",
       "    'skripsi jg dong pak, ih bhaya tau pak bimbingan klo scara lgsungg:)) krna klo bmbingan online ga efektif pak sriuss dahh hhhhe3x',\n",
       "    'Pak, Tugas Akhir / Skripsi kaga mau sekalian pak?',\n",
       "    'Skripsi an di tiada in gak pak? Di tahun ini Aja? Ganti Apa proposal Aja ya :(( please :\"',\n",
       "    'Bagus lah',\n",
       "    'Mantap pak.karena kondisi sekarang sudah memberikan libur di rumah atau online dari rumah',\n",
       "    'tolong pak, dosen saya gaptek.. terus revisi skripsian saya gmna? secara online pun gakbisa , ditelfon juga gabisa. di sms gak bales. ditemuin jg gamau... terus skripsi saya gmna ya pak tolong ',\n",
       "    'Apakah skripsi juga ditiadakan ???',\n",
       "    'Solali lali\\nOla ola laa\\nSMA hepiii\\nSMK oraaa',\n",
       "    'Bagaimana dengan Penundaan Cicilan Ke Bank atau Leasing kendaraan atau Rumah, Sepertinya Pihak BANK atau Leasing belum Memberitahukan kepada Nasabah atau Debitur ?',\n",
       "    'ujian skripsi sekalian, pak',\n",
       "    'Pak skripsi nggak sekalian ditiadain juga pak? Bingung nihh pak mau penelitian kagak bisa juga pak',\n",
       "    'Alhamdulillah,smoga uk strusnya. Ckup pola ujian spt jmn dlu sj. Buktinya yg jd PRSIDEN PRTAMA SP KE BPK PRESIDEN JOKOWI TRCINTA,Jg tdk ada yg ikut ujian nas,sdh trbukti bs jd org2 hebat bhkan Presiden. Sy jg skloh hax d kpg bs juara umum trus,bs msuk SMA fav,bs jd PNS llus MURNI',\n",
       "    'Yang mahasiswa bikin tagar saja #tiadakanskripsi',\n",
       "    'Pak skripsinya ga sekalian ya? Kami ngambil data juga kan ketemu sama orang yang bukan 1 atau 2 orang:( bimbingan juga online pak, ga efektif:(',\n",
       "    'Kalau ujian praktek nya gimana smk saya belum. Tapi kalau un nya sudah selasai',\n",
       "    '@b4kt3riii',\n",
       "    'Hari² semakin banyak Korban mati dan korban tertular...sudahlah pak presiden LOCK DOWN INDONESIA atau daerah merah sekarang juga...cobalah gunakan nurani anda, terimalah saran\" tim pakar IDI untuk membendung wabah ini..#LockdownNow #lockdownindonesia',\n",
       "    'Ujian komprehensif utk mahasiswa akhir juga dong pak ',\n",
       "    'Pak adain aplikasi di hp buat alarm Covid - 19 agar membantu yang terjangkit langsung di tangani oleh dokter dari pemerintah jadi nggak ngerasain di tolak sana sini (di tolak di bidan lah dan di rumah sakit kecil gth intiny)',\n",
       "    'Yess gak ada ya putus dengan alasan mau fokus UN',\n",
       "    'Pak tolong kami para medis\\nYang seharusnya Masker sekali pakai dan sekarang tidak lagi, 1 masker untuk 1 minggutisu saja yg diganti2 namun maskernya pemakaian untuk seminggu#curahanHatiMedis',\n",
       "    'Sidang skripsi,sidang KP,sidang DPR yg isinya cuma orang molor semua jg ditiadakan pak,\\nTolong',\n",
       "    'Kita kuliah online kualitas belajar menurun tapi uang semster kaga turun kwkwkwk',\n",
       "    'Kalau skripsi jangan dihapus ya, Pak. Biarkan ia tetap ada.',\n",
       "    'Kasian para cewe gada alasan buat bilang ke cowoknya “ aku mau konsen ke UN ,kita udahan ya “',\n",
       "    'Aku doain semoga skripsi menyusul ditiadakan. Mau ora? @71Dimas',\n",
       "    'Yey, gajadi diputusin gara² mau fokus UN dong pak',\n",
       "    'PAK SKRIPSI JUGA DONG. GIMANA MAU PENELITIAN DAH???',\n",
       "    'Terimakasih pak @jokowi dan para menteri atas kebijakannya.. semoga bisa menjadikan kami lebih baik dan sadar akan keadaan saat ini.. semoga dampak virus ini segera hilang dan semua kegiatan kembali seperti biasa... Kami turut prihatin',\n",
       "    'Semangat pak jokowi, saya tau bapak berjuang keras demi rakyat Indonesia meskipun mungkin masih banyak keluhan dan kekurangan sana sini, selalu sehat njeh pak',\n",
       "    'wah kok gk pas jaman biyen ae iki @ariessamudraw',\n",
       "    'Alumni 2020 Lulus jalur Virus Corona ',\n",
       "    'Pak skripsi juga ditiadakan dong pak hehe',\n",
       "    'ga ada gitu pak, mahasiswa akhir yg lg penelitian dan mengurus urusan dikampus utk seminar ditangguhkan aja di smt depan tanpa bayar spp. pusink ini bos mau penelitian ke lapangan temu 90an sampel blm lg ngurus suratnya kampus tutup malah bentar lg smt baru n lebaran. hadeuh',\n",
       "    'Nasib mahasiswa gimana pak? Apalagi bagi mereka yang tidak bisa bimbingan secara online, waktu 14hari yg seharusnya dipake buat bimbingan eh malah harus #dirumahaja gegara corona, sidang dan wisuda mereka jadi terhambat',\n",
       "    'Untuk skripsi bagaimana pa @jokowi ? Terutama bagi yang sedang melakukan penelitian di sekolah sekarang ? Masa iya harus wisuda taun deoan dan gagal cumlaude pa ',\n",
       "    'Berita baik tentang corona hari ini pak. Semoga ini semua benar',\n",
       "    'tugas online yg diberikan dosen juga dibatalkan pak',\n",
       "    'Pak bagaimana nasib mhs yg mengambil skripsi dan objek penelitiannya peserta didik SMA sedangkan anak SMAnya libur  baru kali libur tapi saya sedih',\n",
       "    'Buat yang udh beli buku fokus un sama ikutan les buat un,  anda mantap sekali.',\n",
       "    'Lucky you lil brother.',\n",
       "    'sidang skripsi kapan ditiadakan? ',\n",
       "    'pak mohon yang sedang mengerjakarkan tugas akhir/skripsi bagaimana ya? apakah ditiadakan atau diganti dengan sidang onlen? kalau begini nanti mahasiswa tingkat akhir kapan lulusnya pak ',\n",
       "    '@dangharbor kaga un lu ye ena bnr wkwkwk',\n",
       "    'Pak skripsi paaaak',\n",
       "    'Pa skripsi juga pa. Kita-kita ga bisa ambil data ini ',\n",
       "    'Skripsi juga pak tolong di tiadakan, atau seminar dan sindangnya di tiadakan jdi hanya ngumpulin skripsi nya aja pakkk, orng tua telah mendesak agar cept wisuda pak.',\n",
       "    'Mohon sekiranya Bapak bersedia untuk membebaskan biaya kuota internet, agar anak sekolah bisa belajar scr online di Rumah,.. Suwun,..',\n",
       "    'trus apa pengganti UN???',\n",
       "    'Ada lo @mao_lewat',\n",
       "    'Belajar di rumah lebih efektif jika TV Nasional dan swasta mengganti acara sinetron, film, olahraga, game, musik, berita2 seleb, dan acara2 humoris dg video2 yg berisi materi pendidikan SD-SLTA, minimal 3-4 jam se hari. Stasiun TV jangan nyari untung aja.\\n#KitaDirumahAjaYa',\n",
       "    'Itu mah sudah harus dmk\\nBukan sebuah prestasi ',\n",
       "    'gantiin uang bimbel pak',\n",
       "    'Bapak baik banget deh, bikin terharu ',\n",
       "    'Detik²,bimbel,simulasi?\\nApa kabar?',\n",
       "    'Skripsi gimana pak',\n",
       "    'Lockdown jok, sdh banyak daerah yg zona merah. Tegal sdh berani lockdown, knp @DKIJakarta tdk boleh lockdown?',\n",
       "    'Gimna ya? Kya ada rasa kecewa udh dri dulu tradisinya seklh klo mau luls ujian dulu lah ini g ada ujian padlh bru aja megng komptr. Tpi y udhlah hrusnya sneng g ribet lagi mikirin ujian',\n",
       "    'Pak sidang di tiadakan dong pak, gapapa deh nyusun tugas akhir tp gausa sidang atau ga sidangnya perkelompok atau ga bole di wakilin gtu saya sidang ngambil stnk bole diwakilin masa sidang yg ini gaboleh',\n",
       "    'Skripsi hapus dong pak, saya janji pak, kalau skripsi ditiadakan saya gak main mobile legend lagi',\n",
       "    'Ada yg mau beli buku detik2 ga gann masih mulus no minus dijual karna ga jadi UN :v',\n",
       "    'Kasian chika yang nolak badrun kemaren bikin alasan mau focus UN pak',\n",
       "    'Ini respon murid saya Pak \\nhttps://x.com/rrurr15/status/1242376399375069185?s=19…',\n",
       "    'Tadi sempet buat polling di ige sesaat setelah viedo Mas Menteri beredar dan ini respon anak-anak saya ',\n",
       "    'Hmm',\n",
       "    'Luhut Akui Pemerintah Lamban Tangani Virus Corona\\nhttp://gelora.co/2020/03/luhut-akui-pemerintah-lamban-tangani.html…',\n",
       "    'tolong tiadakan tugas belajar di rumah pak,pusing saya pak tiap hari tugas numpuk deadline beda beda bahaya pak bisa depresi\\njadi tolong tiadakan tugas selama belajar di rumah karena corona ini pak,saya mohon banget ini mah sama bapak demi mengurangi beban hidup kami para pelajar',\n",
       "    'Pak mahasiswa semester akhir apa kabar? Kami tidak bisa melanjutkan penelitian Pak. Sekolah dimana-mana libur semua, kampus juga libur. Kasi kejelasanlah Pak.',\n",
       "    'baca komen2nya mewakili angkatan 16 banget wkwkwk',\n",
       "    'Apakah ada kebijakan pemerintah.....potongan bayar SPP yang mulia? Utk anak sekolah yg diliburkan? Terutama swasta.',\n",
       "    'Ujian kompetensi perawat juga kalau bisa di tindakan pak, jangan di undur juli, biar kita para perawat muda ini bisa langsung kerja pak',\n",
       "    'pemerintah indonesia kurang cepat dan sangat lambat dalam menanggani masalah virus corona (covid19) memang berat ujian untuk president @jokowi sudah seharusnya pemerintah bergerak cepat dalam membasmi covid 19 di indonesia agar keadaan beransur baik dan dapat beraktivitas kembali',\n",
       "    'Yang dulu buka akses wisata selebar-lebarnya dan meremehkan ancaman virus sebelum merebak siapa? \\nBakal dituntut banyak orang. Di dunia dan akhirat.',\n",
       "    'Bapak anak kuliah bayar UKT tidak pak? @jokowi',\n",
       "    'Sebelum pemilihan presiden, rajin bagi-bagi bingkisan, sekarang rakyat butuh masker pak, gak ada bantuan dari bapak.....',\n",
       "    'Waaah...kadung bimbel bayar larang, pak.\\nGak sido kemaki yen UN ditiadakan.\\n',\n",
       "    'Pak? Skripsian gimana?\\nYg penelitianya disekolah? Kapan sekolah bisa efektif? Atau kapan skripsi di batalin juga? Atau mala dihapusin? Pak?',\n",
       "    'Ga sekalian pak skripsi ditiadakan? Saya gabisa turun lapangan nihhh huhuu, kapan wisuda kalo kek gini ',\n",
       "    'Skarang mah ga pake UN juga udh pasti lulus ',\n",
       "    'Aku contoh nya yang mau unbk. Ada seneng nya ada sedih nya . Seneng nya kita bisa libur sedih nya udah terlanjur beli buku sukses unbk 2020 :(',\n",
       "    'Kang @BadruddinEmce @emYazzlubnahl @BANG_ISKHAQ kiye lah tembe info A1 wkwkkwk',\n",
       "    'Skripsi gimana pak?',\n",
       "    'Skripsi pripun pak? :(',\n",
       "    'kasian yg udh diputusin dgn alasan nyuruh fokus un pak',\n",
       "    'Bagaimana dengan skripsi pak ? Di tiadakan juga ',\n",
       "    'kasian yang udah putus karena alesan UN pak',\n",
       "    'Kepada adek² siswa/i angkatan 2020.. Selamat anda lulus jalur corona.',\n",
       "    'Trs bagaimana dg kelulusan pak,  apa wisuda juga di tiadakan?',\n",
       "    'Negara tetangga .. menghimbau jika pe duduk masih bepergian jika terkena sakit karna corona biaya rumah sakit disuruh nanggung sendiri Full ... well done #KitaDirumahAjaYa',\n",
       "    'Wah gaswat~ Bisnis bocoran soal UN onlineku bisa gulung tikar ini... ',\n",
       "    'Izin pak kalau utbk gmn ya pak?saya udah gapyear selama setahun semenjak lulus sma biar bisa lulus ptn,pak utbksbmptn 2020 ini gk bisa diringankan kaya unbk?dijadiin bebas tes masuk ptn gt pak?ya allah bahagia dunia akhirat pak seriusannotice pak plis,sendingbuat bapak',\n",
       "    'Di rumah aja nnti kita Mabar',\n",
       "    'UKK untuk SMK 2020 gimana?',\n",
       "    'Sebelum masuk indonesia itu harusnya di cegah.jngn cuma mikirin ekonomi. Kalo udah begini bukan cuma ekonomi.tapi rakyat pada mati',\n",
       "    'tolong untuk tenaga pengajar nya dihimbau juga pak, supaya memberi tugas sewajarnya dan deadline yg masuk akal',\n",
       "    'Dengan penuh hormat pak.\\n\\nTolong juga di tiadakan skripsian, anggota dpr, dan temen yg nongkrong padahal punya rokok tapi ngisep rokok orang.\\n\\nTrims pak. -jajang sarung di cikapundung',\n",
       "    'Dari pada bayar design grafis, mending pakek canva',\n",
       "    'adek saya nangis pak malah gajadi UN gara2 udh niat dan bayar les pula',\n",
       "    'Pak tolong lah tugas tugas juga di tiadakan',\n",
       "    'Kalau terkait dengan Skripsi masih seperti biasakan pak ?',\n",
       "    'Mohon pak presiden sosialisasikan   agar uang steril dari virus covid 19. terimakasih',\n",
       "    \"Kita belajar dari china tentang pencegahan covid 19' bagaimana cara agar uang yg digunakan dimasyarakat steril dari covid 19. Terimakasih\",\n",
       "    'enjoyyyyyy',\n",
       "    'Dilarang berkumpul pak atau mengumpulkan massa.. itu fotonya di ganti sm foto bpk sendiri aja ',\n",
       "    'Lulus jalur corona',\n",
       "    'SKRIPSI SEKALIAN DONG PAK, BIAR ADIL ',\n",
       "    'Mantap pak Jokowi',\n",
       "    'Alhamdulillah. ',\n",
       "    'Boong pak, ga pada belajar di rumah juga',\n",
       "    'Bapa harus menerapkan kebijakan sementara lock down termasuk antar daerah demi nyawa rakyat bapa...',\n",
       "    'Tp jgn kasih pe er banyak2 pak utk anak2 dirumah, soalnya ortu jg WFH jd nambah byk kerjaan mana dikasih waktu utk ngerjain dari gurunya. Jd pada rebutan utk diajarin pelajarannya..',\n",
       "    'Ukom ngga sekalian juga pak?\\nKasian kan yg pengen kerja ngga bs karna harus terhalang ukom diundur, diundurnya lama bgt :\"',\n",
       "    '#RezimJKWPembohong',\n",
       "    \"Assalamu'alaikum bapak presidenku tercinta. Apa tidak mau meniadakan SKRIPSI, KTI, beserta SIDANG-SIDANG nya pak. Mohon dengarkan curahan hati mahasiswa tingkat akhir yang entah nasib nya bagaimana karena kebanyakan sidang belom juga penelitian praktek pak. Suwun njih pak \",\n",
       "    'Tugas mahasiswa gak sekaliak pak?? Gak kena corona malah kena depresi gara² tugas :(',\n",
       "    'Keju mozarella khas malangnya Pak',\n",
       "    'Alhamdulillah pak @jokowi semua ada hikmah dibalik musibah',\n",
       "    'Alhamdulillah']},\n",
       "  'https://x.com/putu_waw/status/1220496942519554048': {'caption': 'Mari bingungkan anak zaman sekarang.\\n#platypus',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1216726296220225536': {'caption': ' : \"Aku itu orangnya kadang suka kayak larutan yang mampu mempertahankan pH.\"\\n : \"Apaan tuh?\"\\n : \"Buffer :v\"\\n#Chemistry',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1212040819391123457': {'caption': '1 / 366 - Leap Year\\n#happynewyear2020',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1208591386821320704': {'caption': 'Koramil 1603 Seririt.',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1205885544732868608': {'caption': 'Sedih sih kalau begini ceritanya\\n@FiersaBesari\\n#SoundofJustice',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1159307146783154176': {'caption': 'Training of Trainer (ToT)\\n\"Optimalisasi Penyediaan Akses Internet untuk Mendukung Sektor Pendidikan dan Pariwisata\".\\n@bahaso',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1151243345110294528': {'caption': 'Lunar Eclipse \\n#GerhanaBulanSebagian\\n#EclipseLunar',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1143386205066629121': {'caption': ' : \"Can we take a picture?\"\\n : \"Of course, gather at mid.\"\\n : \"Take english class!\"\\n\\n#MobileLegends\\n#LagiMales',\n",
       "   'comments': ['Nice pick wkwkkwkwkw']},\n",
       "  'https://x.com/putu_waw/status/1140272844858777602': {'caption': 'Terimakasih telah menjadi ayah kedua kami.\\n•  Wayan Suarsa, S.Pd. •',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1137016268580503553': {'caption': 'X MIA 1 - Vedicience.',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1133278655046492161': {'caption': 'Kalau aku sambat, memangnya kenapa? @nksthi',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1118875128782180353': {'caption': 'Inget masa-masa dimana liat hubungan orang cuma dari status.  #GoodbyeBBM',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1111973639123959808': {'caption': 'Bersatu Merangkai Warna Nusantara | HUT 415 Singaraja',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1056487916334067713': {'caption': '[Late Post]\\nSalam dari Gianyar untuk Smanser. #SMANSERFIRE',\n",
       "   'comments': []},\n",
       "  'https://x.com/putu_waw/status/1041716765611286528': {'caption': 'Serah terima jabatan kepengurusan OSIS SMA Negeri 1 Seririt Masa Bhakti 2017/2018 ke OSIS SMA Negeri 1 Seririt Masa Bhakti 2018/2019. Good luck!',\n",
       "   'comments': []}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.get(FACEBOOK_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_xpath_from_bs4_element(element: PageElement) -> Optional[str]:\n",
    "    try:\n",
    "        components = []\n",
    "        while element:\n",
    "            siblings = element.find_previous_siblings(element.name)\n",
    "            if siblings:  # only add index if there are siblings\n",
    "                index = len(siblings) + 1\n",
    "                components.append(f\"{element.name}[{index}]\")\n",
    "            else:\n",
    "                components.append(f\"{element.name}\")\n",
    "            element = element.parent\n",
    "        result = \"/\" + \"/\".join(reversed(components))\n",
    "        result = result.replace(\"/[document]\", \"\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _get_reels_post_id_fb() -> set:\n",
    "    result = set()\n",
    "    try:\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        reels_anchor = soup.find_all(\"a\", class_=\"x1i10hfl x1qjc9v5 xjbqb8w xjqpnuy xa49m3k xqeqjp1 x2hbi6w x13fuv20 xu3j5b3 x1q0q8m5 x26u7qi x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xdl72j9 x2lah0s xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r x2lwn1j xeuugli xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1ja2u2z x1t137rt x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1q0g3np x87ps6o x1lku1pv x1rg5ohu x1a2a7pz x1n2onr6 xh8yej3\")\n",
    "        for a in reels_anchor:\n",
    "            url = \"/\".join(a['href'].split(\"/\")[:3])\n",
    "            url = f\"{FACEBOOK_BASE_URL}{url}\"\n",
    "            result.add(url)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _show_reels_caption_fb():\n",
    "    see_more_element = webdriver.find_elements(\n",
    "        by=By.XPATH, value=\"//div[contains(text(), 'See more')]\"\n",
    "    )\n",
    "    for element in see_more_element:\n",
    "        try:\n",
    "            element.click()\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_reels_caption_fb():\n",
    "    # show full caption\n",
    "    _show_reels_caption_fb()\n",
    "    time.sleep(1)\n",
    "\n",
    "    caption = \"\"\n",
    "    soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    outer_div = soup.find_all(\"div\", class_=\"xyamay9 x1pi30zi x1swvt13 xjkvuk6\")\n",
    "    for div in outer_div:\n",
    "        spans = div.find_all(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm x17z8epw\")\n",
    "        for span in spans:\n",
    "            caption += span.text\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_replied_more_comments_fb():\n",
    "    replied_element = webdriver.find_elements(\n",
    "        by=By.XPATH, value=\"//span[contains(text(), 'replied')]\"\n",
    "    )\n",
    "    for element in replied_element:\n",
    "        try:\n",
    "            element.click()\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "        # show more comment\n",
    "    more_comment_element = webdriver.find_elements(\n",
    "        by=By.XPATH, value=\"//span[contains(text(), 'more comments')]\"\n",
    "    )\n",
    "    for element in more_comment_element:\n",
    "        try:\n",
    "            element.click()\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _show_reels_comment_fb():\n",
    "    comment_button = webdriver.find_elements(\n",
    "        by=By.XPATH, value=\"//div[@aria-label='Comment']\"\n",
    "    )\n",
    "    for element in comment_button:\n",
    "        try:\n",
    "            element.click()\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_reels_comment_fb(\n",
    "    show_comment: bool = True,\n",
    "    previous_comments: Optional[int] = 0,\n",
    "    current_iteration: Optional[int] = 1,\n",
    "    max_iteration: Optional[int] = 20,\n",
    "):\n",
    "    # show comment section\n",
    "    if show_comment:\n",
    "        _show_reels_comment_fb()\n",
    "        time.sleep(2)\n",
    "\n",
    "    # get captions reels\n",
    "    result = []\n",
    "    soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    outer_span = soup.find_all(\n",
    "        \"span\",\n",
    "        class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u\",\n",
    "    )\n",
    "    for div in outer_span:\n",
    "        divs = div.find_all(\"div\", class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\")\n",
    "        for d in divs:\n",
    "            anchors = d.find_all(\"a\")\n",
    "            replied_username = None\n",
    "            for a in anchors:\n",
    "                if a:\n",
    "                    replied_username = a.text\n",
    "            comment: str = (\" \".join(div.stripped_strings))\n",
    "            if replied_username and comment.startswith(replied_username):\n",
    "                comment = comment[len(replied_username)+1:] # +1 to remove space\n",
    "                result.append(comment)\n",
    "            else:\n",
    "                result.append(comment)\n",
    "\n",
    "    if len(result) > previous_comments and current_iteration < max_iteration:\n",
    "        _show_replied_more_comments_fb()\n",
    "        time.sleep(3)\n",
    "        return get_reels_comment_fb(\n",
    "            show_comment=False,\n",
    "            previous_comments=len(result),\n",
    "            current_iteration=current_iteration + 1,\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _get_video_caption_fb():\n",
    "    caption = \"\"\n",
    "    new_soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    outer_divs = new_soup.find_all(\"div\", class_=\"x1swvt13 x1pi30zi xyamay9\")\n",
    "    for outer_div in outer_divs:\n",
    "        spans = outer_div.find_all(\n",
    "            \"span\",\n",
    "            class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u\",\n",
    "        )\n",
    "        for span in spans:\n",
    "            divs = span.find_all(\n",
    "                \"div\", class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\"\n",
    "            )\n",
    "            for div in divs:\n",
    "                caption += div.text\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _get_video_comment_fb(\n",
    "    previous_comments: Optional[int] = 0,\n",
    "    current_iteration: Optional[int] = 1,\n",
    "    max_iteration: Optional[int] = 20,\n",
    "):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    outer_span = soup.find_all(\n",
    "        \"span\",\n",
    "        class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u\",\n",
    "    )\n",
    "    for span in outer_span:\n",
    "        divs = span.find_all(\n",
    "            \"div\",\n",
    "            class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\",\n",
    "        )\n",
    "        for div in divs:\n",
    "            anchors = div.find_all(\"a\")\n",
    "            replied_username = None\n",
    "            for a in anchors:\n",
    "                if a:\n",
    "                    replied_username = a.text\n",
    "            comment: str = \" \".join(div.stripped_strings)\n",
    "            if replied_username and comment.startswith(replied_username):\n",
    "                comment = comment[len(replied_username) + 1 :]  # +1 to remove space\n",
    "                result.append(comment)\n",
    "            else:\n",
    "                result.append(comment)\n",
    "\n",
    "    logger.info(f\"Total comments found: {len(result)} vs {previous_comments}\")\n",
    "    if len(result) > previous_comments and current_iteration < max_iteration:\n",
    "        _show_replied_more_comments_fb()\n",
    "        time.sleep(3)\n",
    "        return _get_video_comment_fb(\n",
    "            previous_comments=len(result), current_iteration=current_iteration + 1\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_caption_fb():\n",
    "    if \"/videos\" in webdriver.current_url or \"/watch\" in webdriver.current_url:\n",
    "        # can't be used outside, because /videos give different UI result\n",
    "        caption = _get_video_caption_fb()\n",
    "        return caption\n",
    "\n",
    "    if \"/reel\" in webdriver.current_url:\n",
    "        caption = get_reels_caption_fb()\n",
    "        return caption\n",
    "\n",
    "    caption = \"\"\n",
    "    new_soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    outer_divs = new_soup.find_all(\n",
    "        \"div\",\n",
    "        class_=\"x1l90r2v x1pi30zi x1swvt13 x1iorvi4\",\n",
    "        attrs={\"data-ad-preview\": \"message\"},\n",
    "    )\n",
    "    for d in outer_divs:\n",
    "        new_divs = d.find_all(\"div\", class_=\"xu06os2 x1ok221b\")\n",
    "        for div in new_divs:\n",
    "            span = div.find_all(\n",
    "                \"span\",\n",
    "                class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\",\n",
    "            )\n",
    "            for s in span:\n",
    "                caption += s.text\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_post_id_fb(max_posts: int = -1) -> List[str]:\n",
    "    result = set()\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        spans = soup.find_all(\n",
    "            \"span\",\n",
    "            class_=\"x4k7w5x x1h91t0o x1h9r5lt x1jfb8zj xv2umb2 x1beo9mf xaigb6o x12ejxvf x3igimt xarpa2k xedcshv x1lytzrv x1t2pt76 x7ja8zs x1qrby5j\",\n",
    "        )\n",
    "        for span in spans:\n",
    "            a_tags = span.find_all(\n",
    "                \"a\",\n",
    "                class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1sur9pj xkrqix3 xi81zsa x1s688f\",\n",
    "            )\n",
    "            for a_tag in a_tags:\n",
    "                if \"href\" in a_tag.attrs:\n",
    "                    post_id: str = a_tag[\"href\"]\n",
    "                    if post_id.startswith(\"https\"):\n",
    "                        params_idx = post_id.find(\"?\")\n",
    "                        if params_idx != -1:\n",
    "                            result.add(post_id[:params_idx])\n",
    "                        else:\n",
    "                            result.add(post_id)\n",
    "                    else:\n",
    "                        # href not converted into post id\n",
    "                        # need to hover on the link to make it change\n",
    "                        logger.warning(\"Found href not converted into post id\")\n",
    "                        try:\n",
    "                            xpath = _get_xpath_from_bs4_element(a_tag)\n",
    "                            element = webdriver.find_element(By.XPATH, xpath)\n",
    "                            action = ActionChains(webdriver)\n",
    "                            action.move_to_element(element).perform()\n",
    "                            time.sleep(0.3)\n",
    "                        except Exception as e:\n",
    "                            logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "        # search for reels id\n",
    "        reels_id = _get_reels_post_id_fb()\n",
    "        result.update(reels_id)\n",
    "\n",
    "        history.append(len(result))\n",
    "        if max_posts != -1 and len(result) >= max_posts:\n",
    "            logger.info(\"Break because max posts reached\")\n",
    "            break\n",
    "\n",
    "        logger.info(f\"Total post id scraped: {len(result)}\")\n",
    "        if len(history) > 5:\n",
    "            if history[-5] == history[-1]:\n",
    "                logger.info(\"No new post found\")\n",
    "                break\n",
    "\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_comments_fb():\n",
    "    result = list()\n",
    "    history = list()\n",
    "\n",
    "    if \"/videos\" in webdriver.current_url or \"/watch\" in webdriver.current_url:\n",
    "        # can't be used outside, because /videos give different UI result\n",
    "        caption = _get_video_comment_fb()\n",
    "        return caption\n",
    "\n",
    "    if \"/reel\" in webdriver.current_url:\n",
    "        result = get_reels_comment_fb()\n",
    "        return result\n",
    "    \n",
    "    repeat = True\n",
    "    while repeat:\n",
    "        webdriver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "\n",
    "        # click all replied\n",
    "        try:\n",
    "            replied_buttons = webdriver.find_elements(By.XPATH, \"//span[contains(text(), 'replied')]\")\n",
    "            for element in replied_buttons:\n",
    "                element.click()\n",
    "\n",
    "            more_comments_button = webdriver.find_elements(By.XPATH, \"//span[contains(text(), 'more comments')]\")\n",
    "            for element in more_comments_button:\n",
    "                element.click()\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"xwib8y2 xn6708d x1ye3gou x1y1aw1k\")\n",
    "\n",
    "        history.append(len(divs))\n",
    "        logger.info(f\"Searching more comments, found: {len(divs)}\")\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new comments found\")\n",
    "                break\n",
    "\n",
    "    logger.info(\"Start scrapping comments\")\n",
    "    soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    spans = soup.find_all(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u\")\n",
    "    for span in spans:\n",
    "        divs = span.find_all(\"div\", class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\")\n",
    "        for div in divs:\n",
    "            anchors = div.find_all(\"a\")\n",
    "            replied_username = None\n",
    "            for a in anchors:\n",
    "                if a:\n",
    "                    replied_username = a.text\n",
    "            comment: str = (\" \".join(div.stripped_strings))\n",
    "            if replied_username and comment.startswith(replied_username):\n",
    "                comment = comment[len(replied_username)+1:] # +1 to remove space\n",
    "                result.append(comment)\n",
    "            else:\n",
    "                result.append(comment)\n",
    "    logger.info(f\"Total comments scraped: {len(result)}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_facebook(user_id, max_posts=-1):\n",
    "    dataset = Dataset()\n",
    "\n",
    "    webdriver.get(f\"{FACEBOOK_BASE_URL}/{user_id}\")\n",
    "    list_post_id = get_post_id_fb(max_posts=max_posts)\n",
    "    for url in list_post_id:\n",
    "        logger.info(f\"Scraping post: {url}\")\n",
    "        webdriver.get(url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        caption = get_caption_fb()\n",
    "        comments = get_comments_fb()\n",
    "        post = Post(caption=caption, comments=comments)\n",
    "        dataset.data.data.update({url: post})\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-24 00:41:29.559] [INFO] [get_post_id_fb] args: (), kwargs: {'max_posts': -1}\n",
      "[2024-10-24 00:41:30.187] [WARNING] Found href not converted into post id\n",
      "[2024-10-24 00:41:30.994] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:31.373] [INFO] Total post id scraped: 1\n",
      "[2024-10-24 00:41:32.117] [WARNING] Found href not converted into post id\n",
      "[2024-10-24 00:41:32.724] [WARNING] Found href not converted into post id\n",
      "[2024-10-24 00:41:33.341] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:33.792] [INFO] Total post id scraped: 2\n",
      "[2024-10-24 00:41:34.607] [WARNING] Found href not converted into post id\n",
      "[2024-10-24 00:41:35.272] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:35.690] [INFO] Total post id scraped: 4\n",
      "[2024-10-24 00:41:36.420] [WARNING] Found href not converted into post id\n",
      "[2024-10-24 00:41:37.027] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:37.455] [INFO] Total post id scraped: 4\n",
      "[2024-10-24 00:41:38.297] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:38.672] [INFO] Total post id scraped: 5\n",
      "[2024-10-24 00:41:39.393] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:39.777] [INFO] Total post id scraped: 5\n",
      "[2024-10-24 00:41:40.610] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:41.097] [INFO] Total post id scraped: 5\n",
      "[2024-10-24 00:41:41.871] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:42.388] [INFO] Total post id scraped: 5\n",
      "[2024-10-24 00:41:43.347] [INFO] [_get_reels_post_id_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:43.853] [INFO] Total post id scraped: 5\n",
      "[2024-10-24 00:41:43.857] [INFO] No new post found\n",
      "[2024-10-24 00:41:43.857] [INFO] Scraping post: https://web.facebook.com/putu.widyantara.3/posts/pfbid06ykeRc5eYQ39ovDYB3vBeS1tCYAK6Q5bNZBUm6FxRikd9w586bGuZzzhrg6GSXpQl\n",
      "[2024-10-24 00:41:50.345] [INFO] [get_caption_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:50.649] [INFO] [get_comments_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:41:51.237] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:51.807] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:52.363] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:52.946] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:53.522] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:54.197] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:54.773] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:55.389] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:56.024] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:56.667] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:57.321] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:41:57.321] [INFO] No new comments found\n",
      "[2024-10-24 00:41:57.321] [INFO] Start scrapping comments\n",
      "[2024-10-24 00:41:57.587] [INFO] Total comments scraped: 0\n",
      "[2024-10-24 00:41:57.587] [INFO] Scraping post: https://web.facebook.com/putu.widyantara.3/posts/pfbid023zyuMmC2RpMeFtEGa71jgTbvrBGru9kNX1QHJXDQWej8cQF9mgUm9FGA6uEfQcSBl\n",
      "[2024-10-24 00:42:04.888] [INFO] [get_caption_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:05.167] [INFO] [get_comments_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:06.001] [INFO] Searching more comments, found: 4\n",
      "[2024-10-24 00:42:06.588] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:07.287] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:07.876] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:08.486] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:09.137] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:09.817] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:10.500] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:11.199] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:11.857] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:12.484] [INFO] Searching more comments, found: 6\n",
      "[2024-10-24 00:42:12.487] [INFO] No new comments found\n",
      "[2024-10-24 00:42:12.487] [INFO] Start scrapping comments\n",
      "[2024-10-24 00:42:12.937] [INFO] Total comments scraped: 6\n",
      "[2024-10-24 00:42:12.937] [INFO] Scraping post: https://web.facebook.com/tuti.andayani/posts/pfbid0d7mdbWNL2LSrnXwXDRZ76K78UXe2v18GafUXj5JrJSgLTcRu39CUu3M2jAigyDrZl\n",
      "[2024-10-24 00:42:21.547] [INFO] [get_caption_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:21.822] [INFO] [get_comments_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:22.537] [INFO] Searching more comments, found: 2\n",
      "[2024-10-24 00:42:23.157] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:23.765] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:24.331] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:24.977] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:25.637] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:26.297] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:27.102] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:27.767] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:28.448] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:29.171] [INFO] Searching more comments, found: 5\n",
      "[2024-10-24 00:42:29.172] [INFO] No new comments found\n",
      "[2024-10-24 00:42:29.172] [INFO] Start scrapping comments\n",
      "[2024-10-24 00:42:29.498] [INFO] Total comments scraped: 5\n",
      "[2024-10-24 00:42:29.498] [INFO] Scraping post: https://web.facebook.com/putu.widyantara.3/posts/pfbid0kjXR3JGhHJg19pFUYZsu61ZUDrinnnbM8WukZUsAG2mow1VomTa7t3dph9J1UtUCl\n",
      "[2024-10-24 00:42:36.451] [INFO] [get_caption_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:36.807] [INFO] [get_comments_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:37.517] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:38.146] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:38.807] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:39.547] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:40.198] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:40.857] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:41.522] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:42.179] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:42.807] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:43.467] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:44.112] [INFO] Searching more comments, found: 1\n",
      "[2024-10-24 00:42:44.112] [INFO] No new comments found\n",
      "[2024-10-24 00:42:44.112] [INFO] Start scrapping comments\n",
      "[2024-10-24 00:42:44.417] [INFO] Total comments scraped: 1\n",
      "[2024-10-24 00:42:44.417] [INFO] Scraping post: https://web.facebook.com/itenpradya/posts/pfbid024mYFYT85Ciy1Trab4q3VzHYAsbKZ96TgihcviTferr71B4LZ8NLQ9iFapvnS5kthl\n",
      "[2024-10-24 00:42:50.879] [INFO] [get_caption_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:51.197] [INFO] [get_comments_fb] args: (), kwargs: {}\n",
      "[2024-10-24 00:42:51.837] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:52.461] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:53.178] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:53.837] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:54.492] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:55.117] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:55.747] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:56.377] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:56.977] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:57.610] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:58.231] [INFO] Searching more comments, found: 0\n",
      "[2024-10-24 00:42:58.231] [INFO] No new comments found\n",
      "[2024-10-24 00:42:58.231] [INFO] Start scrapping comments\n",
      "[2024-10-24 00:42:58.502] [INFO] Total comments scraped: 0\n"
     ]
    }
   ],
   "source": [
    "facebook_dataset = scraping_facebook(\"putu.widyantara.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://web.facebook.com/putu.widyantara.3/posts/pfbid06ykeRc5eYQ39ovDYB3vBeS1tCYAK6Q5bNZBUm6FxRikd9w586bGuZzzhrg6GSXpQl': {'caption': 'Melepas rasa penat setelah UAS 1... Traveling to Lovina',\n",
       "   'comments': []},\n",
       "  'https://web.facebook.com/putu.widyantara.3/posts/pfbid023zyuMmC2RpMeFtEGa71jgTbvrBGru9kNX1QHJXDQWej8cQF9mgUm9FGA6uEfQcSBl': {'caption': '[Late Post]Serah terima jabatan kepengurusan OSIS SMA Negeri 1 Seririt Masa Bhakti 2017/2018 ke OSIS SMA Negeri 1 Seririt Masa Bhakti 2018/2019. Good luck!',\n",
       "   'comments': ['Adikku mn kok gk klhtn yah hehe',\n",
       "    'Ada kok Bu Herlina Wati , no 8 dari kanan',\n",
       "    'bes cenik2 sing tpuk',\n",
       "    'Pt jadi osis y...',\n",
       "    'Ndak Om Artana Putu , itu dokumentasi dr pelantikan OSIS masa bhakti 2018/2019, nnti klo di tahunnya Putu 2019/2020.',\n",
       "    'yy...mudah2n nti trpilih jdi osis...ikuti j kgiatn2 osis...']},\n",
       "  'https://web.facebook.com/tuti.andayani/posts/pfbid0d7mdbWNL2LSrnXwXDRZ76K78UXe2v18GafUXj5JrJSgLTcRu39CUu3M2jAigyDrZl': {'caption': 'Selamat bulan bahasa Tetap solid tim #ksptiksmanser',\n",
       "   'comments': ['Asik Kayaknya bukan cuma pascal aja sekarang I Wayan Widiastina',\n",
       "    'hhmmm sudah banyak berubah sepertinya yaa',\n",
       "    'Hahahaha hai senior Rama Danuartha I Wayan Widiastina apa kbrnya?  Trimakasih dedikasi bwt smanser Pascal tetap dihati tapi perkembangan zaman tetap ikuti',\n",
       "    'Waauuu kerenn tik',\n",
       "    'pang rame den']},\n",
       "  'https://web.facebook.com/putu.widyantara.3/posts/pfbid0kjXR3JGhHJg19pFUYZsu61ZUDrinnnbM8WukZUsAG2mow1VomTa7t3dph9J1UtUCl': {'caption': 'Rahajeng Nyepi Tahun Baru Caka 1939. Ngiring ngelaksanayang Catur Brata penyepian.',\n",
       "   'comments': ['']},\n",
       "  'https://web.facebook.com/itenpradya/posts/pfbid024mYFYT85Ciy1Trab4q3VzHYAsbKZ96TgihcviTferr71B4LZ8NLQ9iFapvnS5kthl': {'caption': '121',\n",
       "   'comments': []}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa',\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.get(TIKTOK_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_need_captcha_tiktok():\n",
    "    try:\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        captcha = soup.find(\"div\", class_=\"TUXModal captcha-verify-container\")\n",
    "        if captcha:\n",
    "            logger.warning(\"CAPTCHA detected\")\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def _await_for_captcha_resolved_tiktok():\n",
    "    while _is_need_captcha_tiktok():\n",
    "        logger.warning(\"Pending process. Please resolve CAPTCHA...\")\n",
    "        time.sleep(5)\n",
    "    logger.info(\"CAPTCHA resolved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_caption_tiktok():\n",
    "    _await_for_captcha_resolved_tiktok()\n",
    "    soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "    h1 = soup.find_all(\"h1\", class_=\"css-1fbzdvh-H1Container ejg0rhn1\")\n",
    "    caption = \"\"\n",
    "    for h in h1:\n",
    "        caption += h.text\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def get_comments_tiktok():\n",
    "    result = set()\n",
    "    history = list()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            _await_for_captcha_resolved_tiktok()\n",
    "\n",
    "            # scrolling\n",
    "            div_elements = webdriver.find_element(\n",
    "                By.CSS_SELECTOR, \"div.css-1qp5gj2-DivCommentListContainer.ekjxngi3\"\n",
    "            )\n",
    "            webdriver.execute_script(\"arguments[0].scrollTop += 300;\", div_elements)\n",
    "            time.sleep(2)\n",
    "\n",
    "            # view replies\n",
    "            replies = webdriver.find_elements(\n",
    "                By.CSS_SELECTOR, \"p.css-1flplee-PReplyActionText.eo72wou4\"\n",
    "            )\n",
    "            for r in replies:\n",
    "                try:\n",
    "                    button_status = r.get_attribute(\"data-e2e\")\n",
    "                    if button_status != \"comment-hide\":\n",
    "                        r.click()\n",
    "                except Exception as e:\n",
    "                    logger.error(str(e).split(\"\\n\")[0])\n",
    "        except Exception as e:\n",
    "            logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\", class_=\"css-xm2h10-PCommentText e1g2efjf6\")\n",
    "        for p in paragraphs:\n",
    "            result.add(p.text)\n",
    "\n",
    "        logger.info(f\"Total comment scraped: {len((result))}\")\n",
    "        history.append(len(result))\n",
    "\n",
    "        if len(history) > 10:\n",
    "            if history[-10] == history[-1]:\n",
    "                logger.info(\"No new comment found\")\n",
    "                break\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def next_post_tiktok():\n",
    "    try:\n",
    "        next_button = webdriver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"button.css-1s9jpf8-ButtonBasicButtonContainer-StyledVideoSwitch.e11s2kul11\",\n",
    "        )\n",
    "        next_button.click()\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "\n",
    "\n",
    "@log_func\n",
    "def has_next_post_tiktok() -> bool:\n",
    "    try:\n",
    "        # next button\n",
    "        next_button = webdriver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"button.css-1s9jpf8-ButtonBasicButtonContainer-StyledVideoSwitch.e11s2kul11\",\n",
    "        )\n",
    "        # return True if disabled attribute is None\n",
    "        return next_button.get_attribute(\"disabled\") == None\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_func\n",
    "def show_first_post_tiktok():\n",
    "    try:\n",
    "        _await_for_captcha_resolved_tiktok()\n",
    "        soup = BeautifulSoup(webdriver.page_source, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"css-1uqux2o-DivItemContainerV2 e19c29qe17\")\n",
    "\n",
    "        xpath =_get_xpath_from_bs4_element(divs[0])\n",
    "        element = webdriver.find_elements(By.XPATH, xpath)\n",
    "        element[0].click()\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_single_post_data_tiktok(dataset: Dataset):\n",
    "    _await_for_captcha_resolved_tiktok()\n",
    "    caption = get_caption_tiktok()\n",
    "    comments = get_comments_tiktok()\n",
    "    post = Post(caption=caption, comments=comments)\n",
    "    dataset.data.data.update({webdriver.current_url: post})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_tiktok(username: str, max_posts: Optional[int] = -1) -> Dataset:\n",
    "    try:\n",
    "        if max_posts == 0:\n",
    "            return Dataset()\n",
    "\n",
    "        result = Dataset()\n",
    "        url = f\"{TIKTOK_BASE_URL}/@{username}\"\n",
    "        webdriver.get(url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        show_first_post_tiktok()\n",
    "        _get_single_post_data_tiktok(result)\n",
    "        max_posts -= 1\n",
    "        \n",
    "        if max_posts == -1:\n",
    "            while has_next_post_tiktok():\n",
    "                next_post_tiktok()\n",
    "                time.sleep(2)\n",
    "                _get_single_post_data_tiktok(result)\n",
    "        else:\n",
    "            while max_posts and has_next_post_tiktok():\n",
    "                next_post_tiktok()\n",
    "                max_posts -= 1\n",
    "                time.sleep(2)\n",
    "                _get_single_post_data_tiktok(result)\n",
    "            if max_posts:\n",
    "                logger.warning(\"Total post less than expected\")\n",
    "\n",
    "        # stats\n",
    "        scraped_posts = len(result.data.data)\n",
    "        scraped_comments = sum(len(post.comments) for post in result.data.data.values())\n",
    "\n",
    "        logger.info(f\"Total post scraped: {scraped_posts}\")\n",
    "        logger.info(f\"Total comments scraped: {scraped_comments}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(str(e).split(\"\\n\")[0])\n",
    "        return Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-25 16:23:46.845] [INFO] [show_first_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:46.846] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:46.939] [WARNING] CAPTCHA detected\n",
      "[2024-10-25 16:23:46.940] [WARNING] Pending process. Please resolve CAPTCHA...\n",
      "[2024-10-25 16:23:52.008] [WARNING] CAPTCHA detected\n",
      "[2024-10-25 16:23:52.009] [WARNING] Pending process. Please resolve CAPTCHA...\n",
      "[2024-10-25 16:23:57.101] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:23:57.510] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:57.656] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:23:57.657] [INFO] [get_caption_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:57.658] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:57.754] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:23:57.860] [INFO] [get_comments_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:57.860] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:23:58.030] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:00.205] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:00.205] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:00.424] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:02.639] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:02.639] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:02.749] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:04.908] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:04.909] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:05.053] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:07.249] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:07.250] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:07.405] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:09.582] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:09.582] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:09.738] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:11.922] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:11.923] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:12.069] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:14.347] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:14.348] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:14.507] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:16.645] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:16.646] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:16.771] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:18.899] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:18.900] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:19.008] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:21.211] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:21.211] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:21.327] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:23.474] [INFO] Total comment scraped: 12\n",
      "[2024-10-25 16:24:23.475] [INFO] No new comment found\n",
      "[2024-10-25 16:24:23.483] [INFO] [has_next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:23.502] [INFO] [next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:25.725] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:25.840] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:25.841] [INFO] [get_caption_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:25.841] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:25.953] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:26.167] [INFO] [get_comments_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:26.168] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:26.244] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:28.370] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:28.371] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:28.486] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:30.607] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:30.608] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:30.717] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:32.894] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:32.894] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:33.017] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:35.183] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:35.183] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:35.303] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:37.484] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:37.485] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:37.632] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:39.820] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:39.822] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:39.947] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:42.197] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:42.197] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:42.306] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:44.469] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:44.469] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:44.580] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:46.758] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:46.759] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:46.892] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:49.056] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:49.057] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:49.177] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:51.305] [INFO] Total comment scraped: 2\n",
      "[2024-10-25 16:24:51.306] [INFO] No new comment found\n",
      "[2024-10-25 16:24:51.313] [INFO] [has_next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:51.329] [INFO] [next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:53.545] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:53.669] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:53.671] [INFO] [get_caption_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:53.671] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:53.799] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:53.894] [INFO] [get_comments_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:53.895] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:54.072] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:56.264] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:24:56.265] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:56.404] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:24:58.574] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:24:58.575] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:24:58.723] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:00.888] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:00.889] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:01.007] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:03.161] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:03.162] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:03.304] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:05.417] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:05.419] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:05.529] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:07.667] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:07.667] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:07.775] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:10.000] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:10.001] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:10.118] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:12.263] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:12.265] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:12.371] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:14.491] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:14.491] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:14.572] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:16.711] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:16.711] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:16.861] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:18.976] [INFO] Total comment scraped: 3\n",
      "[2024-10-25 16:25:18.977] [INFO] No new comment found\n",
      "[2024-10-25 16:25:18.986] [INFO] [has_next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:19.014] [INFO] [next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:21.311] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:21.463] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:21.465] [INFO] [get_caption_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:21.466] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:21.620] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:21.766] [INFO] [get_comments_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:21.767] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:21.976] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:24.355] [INFO] Total comment scraped: 20\n",
      "[2024-10-25 16:25:24.356] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:24.531] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:26.761] [INFO] Total comment scraped: 24\n",
      "[2024-10-25 16:25:26.762] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:26.889] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:29.119] [INFO] Total comment scraped: 24\n",
      "[2024-10-25 16:25:29.120] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:29.284] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:31.492] [INFO] Total comment scraped: 24\n",
      "[2024-10-25 16:25:31.493] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:31.639] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:33.857] [INFO] Total comment scraped: 24\n",
      "[2024-10-25 16:25:33.858] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:33.952] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:36.139] [INFO] Total comment scraped: 24\n",
      "[2024-10-25 16:25:36.139] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:36.294] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:38.553] [INFO] Total comment scraped: 43\n",
      "[2024-10-25 16:25:38.554] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:38.728] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:40.932] [INFO] Total comment scraped: 43\n",
      "[2024-10-25 16:25:40.932] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:41.087] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:43.296] [INFO] Total comment scraped: 43\n",
      "[2024-10-25 16:25:43.296] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:43.545] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:45.761] [INFO] Total comment scraped: 43\n",
      "[2024-10-25 16:25:45.762] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:45.915] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:48.089] [INFO] Total comment scraped: 43\n",
      "[2024-10-25 16:25:48.090] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:48.242] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:50.471] [INFO] Total comment scraped: 43\n",
      "[2024-10-25 16:25:50.471] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:50.654] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:53.025] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:25:53.026] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:53.170] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:55.377] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:25:55.377] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:55.518] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:25:57.716] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:25:57.718] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:25:57.874] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:00.101] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:00.101] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:00.310] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:02.522] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:02.523] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:02.712] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:04.933] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:04.934] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:05.105] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:07.290] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:07.291] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:07.465] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:09.756] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:09.757] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:09.969] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:12.270] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:12.270] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:12.470] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:14.697] [INFO] Total comment scraped: 52\n",
      "[2024-10-25 16:26:14.699] [INFO] No new comment found\n",
      "[2024-10-25 16:26:14.709] [INFO] [has_next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:14.737] [INFO] [next_post_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:17.523] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:17.677] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:17.678] [INFO] [get_caption_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:17.679] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:17.805] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:18.068] [INFO] [get_comments_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:18.069] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:18.304] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:20.496] [INFO] Total comment scraped: 20\n",
      "[2024-10-25 16:26:20.497] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:20.635] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:22.840] [INFO] Total comment scraped: 20\n",
      "[2024-10-25 16:26:22.840] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:23.027] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:25.223] [INFO] Total comment scraped: 20\n",
      "[2024-10-25 16:26:25.225] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:25.401] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:27.588] [INFO] Total comment scraped: 20\n",
      "[2024-10-25 16:26:27.588] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:27.747] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:29.994] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:29.994] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:30.247] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:32.423] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:32.424] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:32.588] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:34.777] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:34.777] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:34.943] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:37.153] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:37.153] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:37.319] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:39.519] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:39.519] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:39.773] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:41.960] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:41.960] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:42.130] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:44.353] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:44.354] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:44.486] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:46.700] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:46.701] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:46.875] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:49.065] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:49.066] [INFO] [_await_for_captcha_resolved_tiktok] args: (), kwargs: {}\n",
      "[2024-10-25 16:26:49.333] [INFO] CAPTCHA resolved\n",
      "[2024-10-25 16:26:51.516] [INFO] Total comment scraped: 40\n",
      "[2024-10-25 16:26:51.517] [INFO] No new comment found\n",
      "[2024-10-25 16:26:51.523] [INFO] Total post scraped: 5\n",
      "[2024-10-25 16:26:51.524] [INFO] Total comments scraped: 109\n"
     ]
    }
   ],
   "source": [
    "tiktok_dataset = scraping_tiktok(\"lanmalajah.id\", max_posts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'https://www.tiktok.com/@lanmalajah.id/video/7428950379175496966': {'caption': 'Wenten gatra becik semeton, sarengin nggih 😇🙏',\n",
       "   'comments': ['mntap',\n",
       "    'Ikut mb',\n",
       "    'daftar siki',\n",
       "    '🥰',\n",
       "    'ikut geg 😁',\n",
       "    'milu',\n",
       "    'mbok ayu cantik salam rahayu cantik',\n",
       "    '🙏🙏🙏',\n",
       "    'rahayu mbok. dimogi state ngemolihan kerahayuan',\n",
       "    'klo ngomong bali halus pke subtittle donk biar ngerti',\n",
       "    'ikut',\n",
       "    'Mbok, niki yang dados daftar sane sampun numbas buku elektronik manten nggih?🥺']},\n",
       "  'https://www.tiktok.com/@lanmalajah.id/video/7425981264143928581': {'caption': 'Ngamargiang swadharma ring krama, nyarengin Kecamatan Denpasar Timur. Matur suksma, dumogi sida state mapikenoh 😇🙏🏻',\n",
       "   'comments': ['🙏', 'rahayu🙏']},\n",
       "  'https://www.tiktok.com/@lanmalajah.id/video/7423733793925696774': {'caption': 'Sapunapi semeton? durus komen nggih 😇  #bali  #budaya  #bahasabali  #belajar  ',\n",
       "   'comments': ['swastyastu, mbok.\\nampura niki, yening dados nunas tata cara sane patut nyobahayang dana punia sane ngeranjing. suksma 🙏',\n",
       "    'becik pisan',\n",
       "    '👏👏👏👏']},\n",
       "  'https://www.tiktok.com/@lanmalajah.id/video/7420406383704624404': {'caption': 'Puniki nggih semeton!  #bahasabali #budayabali #belajarbahasabali #bukubahasabali #mcbali ',\n",
       "   'comments': ['emng ada yg bisa matur dgn ida bhatara',\n",
       "    '🙏🙏🙏🥰',\n",
       "    'Wmbok gek , bisa minta textnya',\n",
       "    'wenten bukune buk',\n",
       "    'keren banget gek👍✌️🙏🥰',\n",
       "    '👌',\n",
       "    'Rahayu 🙏🏻',\n",
       "    'jeg becik pisan niki. suksma sampun berbagi',\n",
       "    'DOT melajah bahasa halus, dije melajah ya👍',\n",
       "    'om Swastyastu mb dados ngicen video tutor ngenter puja tri sandya sadurung panca sembah',\n",
       "    'wonderful🥰',\n",
       "    '👍👍👍🙏🙏🙏',\n",
       "    '👍👌',\n",
       "    'ajahin yg mbk 😭',\n",
       "    'jeg sekadi mantap embok,',\n",
       "    'Nunas vt atur\\npanyembrame rikale medue kacuntakan.',\n",
       "    'mbok brp kamusnya mbok',\n",
       "    'kerenn',\n",
       "    'mantap, rahayu Mbok',\n",
       "    'di karang sndiri bisa om... masak pakek buku, intinya memohon maaf kepada seluruh warga dn saudara mngkin di waktu hidupnya ada salah kata atau tingkah..bla bla.bla',\n",
       "    'becik niki, ngalestariang sor singggih basa bali...',\n",
       "    'duhhh sukeh mbok...',\n",
       "    '👍👍👍👍👍👍',\n",
       "    'bisa bantu seandainya kita sbg pengurus perumahan mewakili warga menyampaikan turut berduka cita atas kematian salah satu warga perumahan di rumah duka🙏',\n",
       "    'mbok ije meli buku pang dueg mebahasa bali',\n",
       "    'dije melajah gek, sekadi mantap',\n",
       "    'mantap',\n",
       "    'mantap 👍',\n",
       "    'napi menawi nika bu?',\n",
       "    'ke masyarakat umum mbok',\n",
       "    'nah Niki wau becik antuk dasar bebaos dumun majeng ring Semeton utawi alit alit jagi nunasan majeng ring sane uning. suksme',\n",
       "    'mbok,klo gombalin cwe pke bahasa bali halus gimana ya?....klo bisa buatin yaa,suksme mbk🙏',\n",
       "    'mantap mbok becik pisan niki,titi bahasa dan tatanan kata2 becik...🙏',\n",
       "    'Suksma ping banget, Gek🥰',\n",
       "    'suksema mb 🙏🙏🙏',\n",
       "    'punapi carane ngamolihang buku elektronik puniku mbok?',\n",
       "    '👍👍👍',\n",
       "    'wenten bukune mbok, kecepetan lewat hp..😅',\n",
       "    'suksme!',\n",
       "    'wenten bukune mbok',\n",
       "    'bahasa madya ngenter paruman desa adat🙏🙏🙏',\n",
       "    'kari nyeneng napi kari maurip?',\n",
       "    'mok punapi carane ngilangan grogi biasane ampun siap nging ritatkala pas meangge sap',\n",
       "    'adi dueg mbok\\ntiang sai ngayah, tp dereng bise mebaos',\n",
       "    'dados tiang unduh lan share ring grup WA Lan Melajah Mbok?',\n",
       "    '👍👍',\n",
       "    '🙏🙏🥰🥰',\n",
       "    'mbokk\\ntolong buatkan tata cara medharma swaka,soalnya di suruh buat tugas video memadik gitu😢',\n",
       "    '👍👍👍👍👍',\n",
       "    'indah banget bahasa alus nok 🥺 rage sing ngidang sama sekali',\n",
       "    'ampure Nike bahasa sekadi matur ring Ida bhatara...',\n",
       "    '🔥👍']},\n",
       "  'https://www.tiktok.com/@lanmalajah.id/video/7418575616343166214': {'caption': 'Rahajeng rahina suci Galungan semeton 🙏😇  #bali  #galungan  #kuningan  #budaya  ',\n",
       "   'comments': ['rahayu mbok sareng sami.',\n",
       "    '🙏🙏🙏👍👍',\n",
       "    'Jeg cuantiq mbok geg niki, pinter base halus.👍👍🙏',\n",
       "    '👍',\n",
       "    'nggih Mogi Rahayu ngemolihang kerahajengan ritatkala nyangra rahina Kuningan',\n",
       "    'Rahajeng gek...😊🙏🙏',\n",
       "    'Astungkara rahayu',\n",
       "    'salam kenal, Rahayu',\n",
       "    'Mbok Cantik Ijin Videonya Tyang Posting Ulang 🙏🙏🙏🙏🙏🙏🙏🙏',\n",
       "    '👍salam Rahayu',\n",
       "    'rahajeng sareng sami🙏🙏',\n",
       "    'MBOK CANTIK RAHAJENG GALUNGAN DAN KUNINGAN SEMOGA HYANG WIDHI WACCA SELALU MEMBERIKAN WARA NUGRAHANYA UNTUK ANDA 🙏🙏🙏🙏🙏🙏🙏🙏🙏',\n",
       "    'inggih Rahayu taler mbok🙏',\n",
       "    'Gek candra tumben busanane....rahayu',\n",
       "    '🥰🙏🏻Rahayu',\n",
       "    '👍👍👍🙏🙏🙏',\n",
       "    'Rahajeng mewali🙏',\n",
       "    'yg benar diksinya \"Dumadak dumadik\" napi \"Dumogi\". Mana yg pas dipakai dlm konteks ucapan di video ini?',\n",
       "    'rahajeng lan rahayu mewali mb geg🙏🥰',\n",
       "    'svaha',\n",
       "    '🥰🥰🥰',\n",
       "    'rahajeng, rahayu',\n",
       "    'mantap Rahayu gex Candra bpak mau beli buku BHS halus Bali🙏🙏',\n",
       "    '😁😁😁',\n",
       "    'rahajeng mbok',\n",
       "    'pingin bisa berbahasa Halus Bali',\n",
       "    'slm Rahayu 🙏',\n",
       "    '👍👍👍🙏',\n",
       "    'suksme mewali mbok ayu chandra, dumogi Sami kenak lan rahayu.',\n",
       "    'Om rahayu Gek. Lukluk hadir/rauh niki',\n",
       "    'dumogi rahayu sareng sami 🙏🙏🙏',\n",
       "    'om swastyastu gek jegeg',\n",
       "    '🙏🙏🙏🙏',\n",
       "    'rahayu',\n",
       "    'rahajeng lan rahayu mewali nggih 🙏🙏',\n",
       "    'rahjeng mewalii🙏',\n",
       "    'Rahayu 🙏',\n",
       "    'Rahayu Gex 🙏',\n",
       "    'Rahayu gek🙏',\n",
       "    'sayang bgtz ga da teks nya🙏🙏🙏']}},\n",
       " 'author': 'Putu Widyantara Artanta Wibawa',\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktok_dataset.data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset = Dataset.from_json(\"dataset.json\")\n",
    "current_dataset.data.data.update(tiktok_dataset.data.data)\n",
    "current_dataset.to_json(\"dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
